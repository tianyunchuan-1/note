## https://coding.imooc.com/learn/list/169.html



# 第4章 最基础的分类算法-k近邻算法 kNN
# 	04-0
# 		可以解决分类（天然可解决多分类问题）、同时可以解决回归（KNeighborsRegressor）
#		思想简单、效果强大
# 		采用欧拉距离
# 		KNN是一个不需要训练过程的模型（可以理解数据本就是模型）
#		底层实现不像我们的模拟代码这么简单，KNN本身在预测的过程中是非常耗时的
# 		计算距离，投票作出决策
# 		超参数：在算法运行前，需要决定的参数 （调参，一般就是指 超参数）
# 		模型参数：算法过程中学习的参数
# 		kNN没有模型参数
# 		kNN的“k” 是典型的超参数
# 		kNN缺点
			# 缺点1. 效率低下 （m个样本，n个特征，预测每一个数据需要 O(m*n)  omn的时间复杂度
			# 缺点2. 高度数据相关
			# 缺点3. 预测结果不具有可解释性（距离近不等于本质相近）
			# 缺点3. 维度灾难（解决方法：降维）
#   非参数算法：kNN、决策树

# 04-1 k近邻算法基础 (22:42)

######## 4-2 scikit-learn中的机器学习算法封装 (22:22)
import numpy as np
import matplotlib.pyplot as plt
raw_data_X = [[3.393533211, 2.331273381],
              [3.110073483, 1.781539638],
              [1.343808831, 3.368360954],
              [3.582294042, 4.679179110],
              [2.280362439, 2.866990263],
              [7.423436942, 4.696522875],
              [5.745051997, 3.533989803],
              [9.172168622, 2.511101045],
              [7.792783481, 3.424088941],
              [7.939820817, 0.791637231]
             ]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

#list 转换为 numpyArray
X_train = np.array(raw_data_X)
y_train = np.array(raw_data_y)


plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1],color = "r" )
plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1],color = "b")

x = np.array([8.09,3.36])   							#> array([8.09, 3.36])
x.shape 												#> (2,)

# kNN_scikit-learn ！！！！
from sklearn.neighbors import KNeighborsClassifier
kNN_classifier = KNeighborsClassifier(n_neighbors=6)    # 1.实例化
kNN_classifier.fit(X_train,y_train)						# 2.fit

# 转换x为维度数据
X_predict = x.reshape(1, -1) 							#> array([[8.09, 3.36]])
X_predict.shape 										#> (1, 2)

kNN_classifier.predict(x.reshape(1,-1))   				# 3.predict



x_1 = np.array([[8.09,3.36],[5.79,4.78],[2,10]]) 		# predict list
kNN_classifier.predict(x_1) 

# plot
plt.scatter(X_train[y_train==0,0],X_train[y_train==0,1],color="r")
plt.scatter(X_train[y_train==1,0],X_train[y_train==1,1],color="b")
plt.scatter(x_1[0],x_1[1],color="g")


######### 4-3 训练数据集，测试数据集 (22:46) [train test split]
# 训练和测试数据集的分离
# 通过测试数据直接判断模型的好坏
# 在模型进入正式生产环境中改进模型
# 我们甚至可能需要花一章的时间来阐述这个问题

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
iris.keys()
#> dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])

X = iris.data
y = iris.target

X.shape 		#> (150, 4)
y.shape 		#> (150,)

# Train_test_split
y
# array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
#        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
#        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
shuffled_indexes = np.random.permutation(len(X))
shuffled_indexes
# array([ 30, 145,  33,   6, 102,  36,  46,  42, 149,  64, 138,  34,  21,
#         27,  41,  73,  67, 107, 110,  86,  75,  43,  65, 100,  63,   2,
#        121, 109,  31, 140,  91, 130,  79,  45,  50,  26, 144,  38,  78,
#         58,  29,  87, 116,  82,  37,  98,  59,  74,  99,  92,  14,  90,
#         56, 118,  69,  18,  47,  66,  25,  10,   5, 134,  22,  13, 126,
#         62,  71, 137,  23,  32, 125, 129, 115, 112, 148,  70,  52, 108,
#        146, 114, 128,  88,  35,  97,  80,  76,  96,  77,  48,  49,  19,
#        120,  24, 119,  53,  84,  93,  83, 132,  55,  51,  54,  12,   1,
#         40, 136, 124, 139,   9,  72,  94,  95, 117,  28,   4,  17,   8,
#          3, 106, 135, 113, 111,  57, 147,  68,  20,  11, 104,  81, 131,
#         85,  39, 105,  44, 101, 133,  89,   0,   7, 127, 123,  16, 142,
#         60, 143,  61, 122, 141, 103,  15])
test_ratio = 0.2
test_size = int(len(X) * test_ratio)
test_size

test_indexes = shuffled_indexes[:test_size]
train_indexes = shuffled_indexes[test_size:]
test_indexes
# array([ 30, 145,  33,   6, 102,  36,  46,  42, 149,  64, 138,  34,  21,
#         27,  41,  73,  67, 107, 110,  86,  75,  43,  65, 100,  63,   2,
#        121, 109,  31, 140])

X_train = X[train_indexes]
y_train = y[train_indexes]

X_test = X[test_indexes]
y_test = y[test_indexes]


# sklearn train_test_split
from sklearn.model_selection import train_test_split
train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=666)

# 1.实例化 
from sklearn.neighbors import KNeighborsClassifier 
knn_clf = KNeighborsClassifier(n_neighbors=6)

# 2. fit
knn_clf.fit(X_train,y_train)

# 3.Predict
y_predict = knn_clf.predict(X_test)

# result
y_predict
# array([1, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 2, 1, 0, 2, 2, 2, 1, 0,
#        2, 0, 1, 1, 0, 1, 2, 2])
sum(y_predict==y_test)
sum(y_predict==y_test) / len(X_test)



######## 4-4 分类准确度 (19:20)
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
digits.keys() 								#> dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])
print(digits.DESCR)

X = digits.data 							#> (1797, 64)
y = digits.target 							#> (1797,)
digits.target_names							#> array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

y[:100]
# array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,
#        2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7,
#        7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6,
#        6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4,
#        6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1])

X[:10]
# array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,
#         15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,
#         12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,
#          0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,
#         10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.],
#        [ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,
#          9.,  0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7.,
#         15., 16., 16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,
#          0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16.,
#         16.,  6.,  0.,  0.,  0.,  0.,  0., 11., 16., 10.,  0.,  0.],
#         ...

some_digit = X[666]
some_digit_image = some_digit.reshape(8,8)
plt.imshow(some_digit_image,cmap = matplotlib.cm.binary) 		#> 可视化图片


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier(n_neighbors=3)

knn_clf.fit(X_train,y_train)
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=3, p=2,
#            weights='uniform')

y_predict = knn_clf.predict(X_test)

# 4. Accuracy_score¶
from sklearn.metrics import accuracy_score

# --- .accuracy_score >>>>>>> 得出y_predict后
accuracy_score(y_test,y_predict)     #> 0.9888888888888889

# --- .score >>>>>>> 直接得出 accuracy
knn_clf.score(X_test,y_test) 		#> 0.9888888888888889

# --- 手工计算 accuracy
sum(y_predict==y_test)/len(X_test)




######## 4-5 超参数 (21:36)
# 超参数：在算法运行前，需要决定的参数 （调参，一般就是指 超参数）
# 模型参数：算法过程中学习的参数
# kNN没有模型参数
# kNN的“k” 是典型的超参数
# 寻找好的超参数
	# 领域知识
	# 经验数值
	# 试验搜索



import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train,y_train)
knn_clf.score(X_test,y_test)

#> 0.9888888888888889

## 寻找最好的 K
best_score = 0.0
best_k = -1
for k in range(1,11):
    knn_clf = KNeighborsClassifier(n_neighbors=k)
    knn_clf.fit(X_train,y_train)
    score = knn_clf.score(X_test,y_test)
    if score > best_score:
        best_k = k
        best_score = score

print("best_k = ", best_k)
print("best_score = ", best_score)

# best_k =  4
# best_score =  0.9916666666666667

## 考虑距离与否
best_score = 0.0
best_k = -1
for method in ["uniform","distance"]:
    for k in range(1,11):
        knn_clf = KNeighborsClassifier(n_neighbors=k,weights=method)
        knn_clf.fit(X_train,y_train)
        score = knn_clf.score(X_test,y_test)
        if score > best_score:
            best_k = k
            best_score = score
            best_method = method

print("best_method = ", method)
print("best_k = ", best_k)
print("best_score = ", best_score)

# best_method =  distance
# best_k =  4
# best_score =  0.9916666666666667

## --- 搜索明科夫斯基p
best_p = -1
best_score = 0.0
best_k = -1
for k in range(1,11):
    for p in range(1,6):
        knn_clf = KNeighborsClassifier(n_neighbors=k,weights="distance",p=p)
        knn_clf.fit(X_train,y_train)
        score = knn_clf.score(X_test,y_test)
        if score > best_score:
            best_k = k
            best_score = score
            best_p = p

print("best_p = ", best_p)
print("best_k = ", best_k)
print("best_score = ", best_score)

# best_p =  2
# best_k =  3
# best_score =  0.9888888888888889

######## 4-6 网格搜索与k近邻算法中更多超参数 (17:24)
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train,y_train)
knn_clf.score(X_test,y_test)
#> 0.9888888888888889

#### [Grid Search]
param_grid = [
    {
        "weights":["uniform"],
        "n_neighbors":[i for i in range(1,11)]
    },
    {
        "weights":["distance"],
        "n_neighbors":[i for i in range(1,11)],
        "p": [i for i in range(1,6)]
    }
]
# 1. kNN 实例化
knn_clf = KNeighborsClassifier()

# 2. Grid Search 实例化,引用参数
from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(knn_clf,param_grid, n_jobs=-1,verbose=2)

# 3. Fit (Grid search)
%%time
grid_search.fit(X_train,y_train)

# GridSearchCV(cv=None, error_score=nan,
#              estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,
#                                             metric='minkowski',
#                                             metric_params=None, n_jobs=None,
#                                             n_neighbors=5, p=2,
#                                             weights='uniform'),
#              iid='deprecated', n_jobs=-1,
#              param_grid=[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
#                           'weights': ['uniform']},
#                          {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
#                           'p': [1, 2, 3, 4, 5], 'weights': ['distance']}],
#              pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
#              scoring=None, verbose=2)

#### ---- Grid search 运行参数查看
grid_search.best_estimator_
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=3, p=3,
#            weights='distance')

#### ---- best_score_
grid_search.best_score_
#> 0.9853862212943633

#### ---- best_params_
#> {'n_neighbors': 3, 'p': 3, 'weights': 'distance'}

#### ---- grid_search.best_estimator_实例化 knn_clf
knn_clf = grid_search.best_estimator_

# 4. Predict
knn_clf.predict(X_test)
# array([8, 1, 3, 4, 4, 0, 7, 0, 8, 0, 4, 6, 1, 1, 2, 0, 1, 6, 7, 3, 3, 6,
#        5, 2, 9, 4, 0, 2, 0, 3, 0, 8, 7, 2, 3, 5, 1, 3, 1, 5, 8, 6, 2, 6,
#        3, 1, 3, 0, 0, 4, 9, 9, 2, 8, 7, 0, 5, 4, 0, 9, 5, 5, 8, 7, 4, 2,
#        ...

knn_clf.score(X_test,y_test)
#> 0.9833333333333333


######## 4-7 数据归一化 (15:27)
#### 解决量纲不同的问题，将所有的数据都映射到同一尺度中
import numpy as np
import matplotlib.pyplot as plt
# ---- 最值归一化 Normalization  适用于分布有明显边界的数据，缺点：受outlier影响较大
x = np.random.randint(0, 100, 100) 
(x - np.min(x)) / (np.max(x) - np.min(x))
X = np.random.randint(0, 100, (50, 2))
X = np.array(X, dtype=float)
X[:,0] = (X[:,0] - np.min(X[:,0])) / (np.max(X[:,0]) - np.min(X[:,0]))
X[:,1] = (X[:,1] - np.min(X[:,1])) / (np.max(X[:,1]) - np.min(X[:,1]))

plt.scatter(X[:,0], X[:,1])

np.mean(X[:,0])
np.std(X[:,0])
np.mean(X[:,1])
np.std(X[:,1])

# ---- 均值方差归一化 Standardization  归一到均值为0，方差为1 的分布中
# 适用于分布没有明显边界的数据，有可能存在极端数据值
X2 = np.random.randint(0, 100, (50, 2))
X2 = np.array(X2, dtype=float)
X2[:,0] = (X2[:,0] - np.mean(X2[:,0])) / np.std(X2[:,0])
X2[:,1] = (X2[:,1] - np.mean(X2[:,1])) / np.std(X2[:,1])

plt.scatter(X2[:,0], X2[:,1])

np.mean(X2[:,0]) 	#> -2.2204460492503132e-17
np.std(X2[:,0])		#> 1.0
np.mean(X2[:,1]) 	#> -2.6645352591003756e-17
np.std(X2[:,1]) 	#> 1.0


######## 4-8 scikit-learn中的Scaler (19:24)
#### memo
	# 需要保存训练数据集的方差、均值
import numpy as np
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=666)

#### ---- scikit-learn >>>>>>> StandardScaler
# StandardScaler 1. 实例化
from sklearn.preprocessing import StandardScaler
standardScaler = StandardScaler()

# StandardScaler 2. Fit
standardScaler.fit(X_train)
#> StandardScaler(copy=True, with_mean=True, with_std=True)
print(standardScaler.mean_)
print(standardScaler.scale_)
# [5.83416667 3.0825     3.70916667 1.16916667]
# [0.81019502 0.44076874 1.76295187 0.75429833]

X_train = standardScaler.transform(X_train)
X_train[:2,:]
# array([[-0.90616043,  0.94720873, -1.30982967, -1.28485856],
#        [-1.15301457, -0.18717298, -1.30982967, -1.28485856]])
X_test_standard = standardScaler.transform(X_test)
X_test_standard[:2,:]
# array([[-0.28902506, -0.18717298,  0.44858475,  0.43859746],
#        [-0.04217092, -0.64092567,  0.78892303,  1.63175932]])

#### Scaler后运行分类
from sklearn.neighbors import KNeighborsClassifier
# 1.实例化
knn_clf = KNeighborsClassifier(n_neighbors=3)

# 2. Fit
knn_clf.fit(X_train,y_train)
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=3, p=2,
#            weights='uniform')

# 3. Predict
knn_clf.score(X_test_standard, y_test)
#> 1.0

# 4-9 更多有关k近邻算法的思考 (10:22)
# 参见 4-0


######### 第5章 线性回归法
	# 5-0
	# 解决回归问题
	# 思想简单、实现容易
	# 许多强大的非线性模型的基础（多项式、逻辑回归、SVM）
	# 结果具有很好的解释性
	# 蕴含及其学子中的很多重要思想
	# 距离：不使用绝对值，使用差值的平方，是因为平方更容易使方程式的可导
	# 找到损失函数（loss function）、效用函数（utility） → 机器学习（参数学习）本质（背后的思想）就是最优化损失函数、效用函数 →最优化原理（还有凸优化）
		# 比如说对最短的路径感兴趣、对最小的生成数值感兴趣、希望总价值是最大的
# 5-1 简单线性回归 (18:06)
# 5-2 最小二乘法 (11:27)
#### 5-3 简单线性回归的实现 (14:09)
## byself
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1., 2., 3., 4., 5.])
y = np.array([1., 3., 2., 3., 5.])

plt.scatter(x, y)
plt.axis([0, 6, 0, 6])
plt.show()

x_mean = np.mean(x)
y_mean = np.mean(y)

num = 0.0
d = 0.0
for x_i, y_i in zip(x, y):
    num += (x_i - x_mean) * (y_i - y_mean)
    d += (x_i - x_mean) ** 2

 a = num/d
 b = y_mean - a * x_mean
 y_hat = a * x + b

 plt.scatter(x, y)
plt.plot(x, y_hat, color='r')
plt.axis([0, 6, 0, 6])
plt.show()

x_predict = 6
y_predict = a * x_predict + b
y_predict
## [SK SimpleLinearRegression1] [简单线性回归]

reg1 = SimpleLinearRegression1()
reg1.fit(x, y)
reg1.predict(np.array([x_predict]))


# 5-4 向量化 (12:02)
# 5-5 衡量线性回归法的指标：MSE，RMSE和MAE (22:45)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
print(boston.DESCR)

boston.feature_names
x = boston.data[:,5] # 只使用房间数量这个特征
y = boston.target
plt.scatter(x, y)
plt.show()

x = x[y<50.0]
y = y[y<50.0]

print(x.shape)
print(y.shape)

plt.scatter(x,y)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

mean_squared_error(y_test, y_predict)
mean_absolute_error(y_test, y_predict)

######## 5-6 最好的衡量线性回归法的指标：R Squared (16:28)
# 最重要
# baseline model
from sklearn.metrics import r2_score
r2_score(y_test, y_predict)


#### 5-7 多元线性回归和正规方程解 (15:58)
#### 5-8 实现多元线性回归 (13:08)
#### 5-9 使用scikit-learn解决回归问题 (12:42)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target
X = X[y <50.0]
y = y[y <50.0]

X.shape

# split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2,random_state=666)

# 实例化
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()

# fit
lin_reg.fit(X_train,y_train)
lin_reg.score(X_test,y_test)

# 模型结果
lin_reg.coef_

"""
array([-1.18919477e-01,  3.63991462e-02, -3.56494193e-02,  5.66737830e-02,
       -1.16195486e+01,  3.42022185e+00, -2.31470282e-02, -1.19509560e+00,
        2.59339091e-01, -1.40112724e-02, -8.36521175e-01,  7.92283639e-03,
       -3.81966137e-01])
"""
lin_reg.intercept_
#> 34.161435496246355

## kNN Regressor 解决回归问题
from sklearn.neighbors import KNeighborsRegressor

knn_reg =  KNeighborsRegressor()
knn_reg.fit(X_train,y_train)
knn_reg.score(X_test,y_test)
#> 0.5865412198300899


from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        "weights": ["uniform"],
        "n_neighbors": [i for i in range(1, 11)]
    },
    {
        "weights": ["distance"],
        "n_neighbors": [i for i in range(1, 11)],
        "p": [i for i in range(1,6)]
    }
]

knn_reg = KNeighborsRegressor()
grid_search = GridSearchCV(knn_reg,param_grid,n_jobs=-1)
grid_search.fit(X_train,y_train)
grid_search.best_params_
#> {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}
grid_search.best_estimator_
"""
KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=None, n_neighbors=7, p=1,
                    weights='distance')
"""
grid_search.best_score_ #>0.652216494152461
grid_search.best_estimator_.score(X_test,y_test) #> 0.7160666820548708       


#### 5-10 线性回归的可解释性和更多思考 (11:53)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()

X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)

lin_reg.coef_
"""
array([-1.05574295e-01,  3.52748549e-02, -4.35179251e-02,  4.55405227e-01,
       -1.24268073e+01,  3.75411229e+00, -2.36116881e-02, -1.21088069e+00,
        2.50740082e-01, -1.37702943e-02, -8.38888137e-01,  7.93577159e-03,
       -3.50952134e-01])
"""
np.argsort(lin_reg.coef_)
#> array([ 4,  7, 10, 12,  0,  2,  6,  9, 11,  1,  8,  3,  5], dtype=int64)
boston.feature_names[np.argsort(lin_reg.coef_)]
#> array(['NOX', 'DIS', 'PTRATIO', 'LSTAT', 'CRIM', 'INDUS', 'AGE', 'TAX', 'B', 'ZN', 'RAD', 'CHAS', 'RM'], dtype='<U7')


######### 第6章 梯度下降法
#### 6-0 
	# 不是一个机器学习算法
	# 是一种基于搜索的最优化方法
	# 作用：最小化一个损失函数
	# 梯度上升法：最大化一个效用函数

	# 梯度下降法需要对数据进行归一化处理
	# 梯度下降法的优势
		# -> 比正规方程求解的速度快 （变量越多，差异越大）
		# -> 但是样本量多的话，梯度下降法也非常耗时，由此产生了随机梯度下降法


#### 6-1 什么是梯度下降法 (16:20)
	# 过程有不确定性，但通常可以依然差不多的来到最优值
	# 牺牲一定的精度换取一定的时间


# 并不是所有函数都有唯一的极值点（导数为零）
	# 局部最优解、全部最优解
# 解决方案
	# 多次运行，随机化初始点
	# 梯度下降法的初始点也是一个超参数

#### 6-2 模拟实现梯度下降法 (20:11)
	# eta值 一般情况下可以设置为 0.01

#### 6-3 线性回归中的梯度下降法 (10:56)
np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)

X = x.reshape(-1, 1)
#### 6-4 实现线性回归中的梯度下降法 (14:06)
#### 6-5 梯度下降法的向量化和数据标准化 (22:14)
	# 使用梯度下降法需要对数据进行归一化处理，
	# 设置最大循环次数
	# 比如说只用三分之的样本就可以达到差不多的精度


#### 6-6 随机梯度下降法 (17:34)
	# 梯度下降法的优势
		# -> 比正规方程求解的速度快 （变量越多，差异越大）
		# -> 但是样本量多的话，梯度下降法也非常耗时，由此产生了随机梯度下降法
	# 学习率非常重要

# 6-7 scikit-learn中的随机梯度下降法 (15:40)
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

from sklearn.model_selection import train_test_split
train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=666)


from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X_train)
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)


from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor()
%time sgd_reg.fit(X_train_standard,y_train)
sgd_reg.score(X_test_standard,y_test)



sgd_reg = SGDRegressor(n_iter_no_change=50)     # n_iter_no_change=50 -> 整个样本要浏览多少次
%time sgd_reg.fit(X_train_standard, y_train)
sgd_reg.score(X_test_standard, y_test)
%time sgd_reg.fit(X_train_standard, y_train)
sgd_reg.score(X_test_standard, y_test)

#### 6-8 如何确定梯度计算的准确性？调试梯度下降法 (16:29)
#### 6-9 有关梯度下降法的更多深入讨论 (08:37)
	# 批量梯度下降法
	# 随机梯度下降法 -> SGD
	# 小批量梯度下降法

	# 随机的意义
	# 	跳出局部最优解
	# 	更快的运行速度
	# 	机器学习梁宇很多算法都要使用随机的特点(随机搜索、随机森林)

######## 第7章 PCA与梯度上升法
#### 7-1 什么是PCA (17:45)
#### 7-2 使用梯度上升法求解PCA问题 (09:10)
#### 7-3 求数据的主成分PCA (20:09)
#### 7-4 求数据的前n个主成分 (17:36)
#### 7-5 高维数据映射为低维数据 (19:29)
import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100, 2))
X[:,0] = np.random.uniform(0., 100., size=100)
X[:,1] = 0.75 * X[:,0] + 3. + np.random.normal(0, 10., size=100)

from playML.PCA import PCA

pca = PCA(n_components=2)
pca.fit(X)
# PCA(n_components=2)
pca.components_
# array([[ 0.77441964,  0.63267228],
#        [-0.63266853,  0.77442271]])
pca = PCA(n_components=1)
pca.fit(X)
# PCA(n_components=1)
X_reduction = pca.transform(X)
X_reduction.shape
# (100, 1)
X_restore = pca.inverse_transform(X_reduction)
X_restore.shape
# (100, 2)
plt.scatter(X[:,0], X[:,1], color='b', alpha=0.5)
plt.scatter(X_restore[:,0], X_restore[:,1], color='r', alpha=0.5)
plt.show()

## sklearn
from sklearn.decomposition import PCA
pca = PCA(n_components=1)
pca.fit(X)
# PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,
#     svd_solver='auto', tol=0.0, whiten=False)
X_reduction = pca.transform(X)
X_reduction.shape
# (100, 1)

pca.components_
# array([[-0.77441967, -0.63267225]])

X_restore = pca.inverse_transform(X_reduction)
X_restore.shape
# (100, 2)

plt.scatter(X[:,0], X[:,1], color='b', alpha=0.5)
plt.scatter(X_restore[:,0], X_restore[:,1], color='r', alpha=0.5)
plt.show()

# 7-6 scikit-learn中的PCA (18:57)
import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100, 2))
X[:,0] = np.random.uniform(0., 100., size=100)
X[:,1] = 0.75 * X[:,0] + 3. + np.random.normal(0, 10., size=100)

plt.scatter(X[:,0],X[:,1])

from sklearn.decomposition import PCA
pca = PCA(n_components=1)
pca.fit(X)
# PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,
#   svd_solver='auto', tol=0.0, whiten=False)

pca.components_
# array([[0.7686356 , 0.63968689]])

X_reduction = pca.transform(X)
X_reduction.shape
# (100, 1)

X_restore = pca.inverse_transform(X_reduction)
X_restore.shape
# (100, 2)


######### datasets.digits -> KNN 
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)

X_train.shape
# (1347, 64)

## kNN 不使用PCA
%%time

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train,y_train)
Wall time: 207 ms
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,
#                      weights='uniform')

knn_clf.score(X_test,y_test)
# 0.9866666666666667

## PCA降维
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(X_train)
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)

%%time
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_reduction,y_train)
# Wall time: 0 ns
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,
#                      weights='uniform')

pca.explained_variance_ratio_
# array([0.14566817, 0.13735469])

pca = PCA(n_components=X_train.shape[1])
pca.fit(X_train)
pca.explained_variance_ratio_
# array([1.45668166e-01, 1.37354688e-01, 1.17777287e-01, 8.49968861e-02,
#        5.86018996e-02, 5.11542945e-02, 4.26605279e-02, 3.60119663e-02,
#        3.41105814e-02, 3.05407804e-02, 2.42337671e-02, 2.28700570e-02,
#        1.80304649e-02, 1.79346003e-02, 1.45798298e-02, 1.42044841e-02,
#        1.29961033e-02, 1.26617002e-02, 1.01728635e-02, 9.09314698e-03,
#        8.85220461e-03, 7.73828332e-03, 7.60516219e-03, 7.11864860e-03,
#        6.85977267e-03, 5.76411920e-03, 5.71688020e-03, 5.08255707e-03,
#        4.89020776e-03, 4.34888085e-03, 3.72917505e-03, 3.57755036e-03,
#        3.26989470e-03, 3.14917937e-03, 3.09269839e-03, 2.87619649e-03,
#        2.50362666e-03, 2.25417403e-03, 2.20030857e-03, 1.98028746e-03,
#        1.88195578e-03, 1.52769283e-03, 1.42823692e-03, 1.38003340e-03,
#        1.17572392e-03, 1.07377463e-03, 9.55152460e-04, 9.00017642e-04,
#        5.79162563e-04, 3.82793717e-04, 2.38328586e-04, 8.40132221e-05,
#        5.60545588e-05, 5.48538930e-05, 1.08077650e-05, 4.01354717e-06,
#        1.23186515e-06, 1.05783059e-06, 6.06659094e-07, 5.86686040e-07,
#        1.71368535e-33, 7.44075955e-34, 7.44075955e-34, 7.15189459e-34])

plt.plot([i for i in range(X_train.shape[1])], 
         [np.sum(pca.explained_variance_ratio_[:i+1]) for i in range(X_train.shape[1])])

pca = PCA(0.95)
pca.fit(X_train)
# PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,
#   svd_solver='auto', tol=0.0, whiten=False)
pca.n_components_
# 28
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)

%%time
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_reduction,y_train)

knn_clf.score(X_test_reduction,y_test)
# 0.98

## 使用PCA对数据进行降维可视化
pca = PCA(n_components=2)
pca.fit(X)
X_reduction = pca.transform(X)

for i in range(10):
    plt.scatter(X_reduction[y==i,0], X_reduction[y==i,1], alpha=0.8)
plt.show()

plt.scatter(X_reduction[y==0,0],X_reduction[y==0,1],alpha=0.3)
plt.scatter(X_reduction[y==1,0],X_reduction[y==1,1],alpha=0.3)
plt.scatter(X_reduction[y==8,0],X_reduction[y==8,1],alpha=0.3)

for i in range(10):
    plt.scatter(X_reduction[y==i,0],X_reduction[y==i,1],alpha=0.8)


#### 7-7 试手MNIST数据集 (12:06)
import numpy as np 

# from sklearn.datasets import fetch_mldata
# mnist = fetch_mldata('MNIST original')
# 在最新版的 sklearn 中，fetch_mldata 被弃用，改为使用 fetch_openml 获得 MNIST 数据集
# 具体见如下代码，后续代码无需改变

from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784')
mnist

X, y = mnist['data'], mnist['target']
X_train = np.array(X[:60000], dtype=float)
y_train = np.array(y[:60000], dtype=float)
X_test = np.array(X[60000:], dtype=float)
y_test = np.array(y[60000:], dtype=float)

X_train.shape
# (60000, 784)
y_train.shape
# (60000,)
X_test.shape
# (10000, 784)
y_test.shape
# (10000,)

## 使用kNN

from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
%time knn_clf.fit(X_train, y_train)

%time knn_clf.score(X_test, y_test)
CPU times: user 11min 5s, sys: 445 ms, total: 11min 5s
Wall time: 11min 6s
# 0.9688

## PCA进行降维
from sklearn.decomposition import PCA 
pca = PCA(0.90)
pca.fit(X_train)
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)
X_train_reduction.shape
# (60000, 87)

knn_clf = KNeighborsClassifier()
%time knn_clf.fit(X_train_reduction, y_train)
# CPU times: user 1.21 s, sys: 12.9 ms, total: 1.22 s
# Wall time: 387 ms
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,
#                      weights='uniform')
%time knn_clf.score(X_test_reduction, y_test)
# CPU times: user 1min 8s, sys: 280 ms, total: 1min 9s
# Wall time: 1min 9s
# 0.9728
# 降维去除了噪音，有可能准确率更高！

# CPU times: user 12.1 s, sys: 123 ms, total: 12.2 s
# Wall time: 12.2 s
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,
#                      weights='uniform')

# 7-8 关于 MNIST 数据集的最新获得方式
# 7-9 使用PCA对数据进行降噪 (10:00)
#### 7-10 人脸识别与特征脸 (13:57)
## 特征脸
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people()
faces.keys()
# dict_keys(['data', 'images', 'target', 'target_names', 'DESCR'])
faces.data.shape
# (13233, 2914)
faces.target_names
# array(['AJ Cook', 'AJ Lamas', 'Aaron Eckhart', ..., 'Zumrati Juma',
#        'Zurab Tsereteli', 'Zydrunas Ilgauskas'], 
#       dtype='<U35')
faces.images.shape
# (13233, 62, 47)
random_indexes = np.random.permutation(len(faces.data))
X = faces.data[random_indexes]
example_faces = X[:36,:]
example_faces.shape
# (36, 2914)

# 特征脸
%%time
from sklearn.decomposition import PCA 
pca = PCA(svd_solver='randomized')
pca.fit(X)
# CPU times: user 3min 24s, sys: 8.6 s, total: 3min 33s
# Wall time: 2min 2s
pca.components_.shape
# (2914, 2914)
plot_faces(pca.components_[:36,:])
def plot_faces(faces):
    
    fig, axes = plt.subplots(6, 6, figsize=(10, 10),
                         subplot_kw={'xticks':[], 'yticks':[]},
    gridspec_kw=dict(hspace=0.1, wspace=0.1)) 
    for i, ax in enumerate(axes.flat):
        ax.imshow(faces[i].reshape(62, 47), cmap='bone')
    plt.show()
    
plot_faces(example_faces)

#更多关于lfw_people数据集
faces2 = fetch_lfw_people(min_faces_per_person=60)
faces2.data.shape
# (1348, 2914)
faces2.target_names
# array(['Ariel Sharon', 'Colin Powell', 'Donald Rumsfeld', 'George W Bush',
#        'Gerhard Schroeder', 'Hugo Chavez', 'Junichiro Koizumi',
#        'Tony Blair'], 
#       dtype='<U17')
len(faces2.target_names)
# 8




########	第8章 多项式回归与模型泛化
#### 8-1 什么是多项式回归 (11:38)
# 什么是多项式回归
import numpy as np 
import matplotlib.pyplot as plt
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, 100)
plt.scatter(x, y)
plt.show()

# 线性回归？
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_predict = lin_reg.predict(X)
plt.scatter(x, y)
plt.plot(x, y_predict, color='r')
plt.show()

# 解决方案， 添加一个特征
X2 = np.hstack([X, X**2])
X2.shape
# (100, 2)
lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()

lin_reg2.coef_
# array([ 0.99870163,  0.54939125])
lin_reg2.intercept_
# 1.8855236786516001


#### 8-2 scikit-learn中的多项式回归与Pipeline (16:26)
scikit-learn中的多项式回归和Pipeline
import numpy as np 
import matplotlib.pyplot as plt
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, 100)

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
poly.fit(X)
X2 = poly.transform(X)
X2.shape
# (100, 3)
X[:5,:]
# array([[ 0.14960154],
#        [ 0.49319423],
#        [-0.87176575],
#        [-1.33024477],
#        [ 0.47383199]])
X2[:5,:]
# array([[ 1.        ,  0.14960154,  0.02238062],
#        [ 1.        ,  0.49319423,  0.24324055],
#        [ 1.        , -0.87176575,  0.75997552],
#        [ 1.        , -1.33024477,  1.76955114],
#        [ 1.        ,  0.47383199,  0.22451675]])

from sklearn.linear_model import LinearRegression

lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()

lin_reg2.coef_
# array([ 0.        ,  0.9460157 ,  0.50420543])
lin_reg2.intercept_
# 2.1536054095953823
## 关于PolynomialFeatures
X = np.arange(1, 11).reshape(-1, 2)
X
# array([[ 1,  2],
#        [ 3,  4],
#        [ 5,  6],
#        [ 7,  8],
#        [ 9, 10]])
poly = PolynomialFeatures(degree=2)
poly.fit(X)
X2 = poly.transform(X)
X2.shape
# (5, 6)
X2
# array([[   1.,    1.,    2.,    1.,    2.,    4.],
#        [   1.,    3.,    4.,    9.,   12.,   16.],
#        [   1.,    5.,    6.,   25.,   30.,   36.],
#        [   1.,    7.,    8.,   49.,   56.,   64.],
#        [   1.,    9.,   10.,   81.,   90.,  100.]])


## Pipeline
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, 100)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

poly_reg = Pipeline([
    ("poly", PolynomialFeatures(degree=2)),
    ("std_scaler", StandardScaler()),
    ("lin_reg", LinearRegression())
])
poly_reg.fit(X, y)
y_predict = poly_reg.predict(X)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()


#### 8-3 过拟合与欠拟合 (14:22)
## 03-Overfitting-and-Underfitting.ipynb 103 KB
## 过拟合和欠拟合
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
plt.scatter(x, y)
plt.show()

## 使用线性回归
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.score(X, y)
# 0.49537078118650091
y_predict = lin_reg.predict(X)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()

from sklearn.metrics import mean_squared_error

y_predict = lin_reg.predict(X)
mean_squared_error(y, y_predict)
# 3.0750025765636577

## 使用多项式回归
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])
poly2_reg = PolynomialRegression(degree=2)
poly2_reg.fit(X, y)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lin_reg', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])
y2_predict = poly2_reg.predict(X)
mean_squared_error(y, y2_predict)
# 1.0987392142417856

plt.scatter(x, y)
plt.plot(np.sort(x), y2_predict[np.argsort(x)], color='r')
plt.show()


poly10_reg = PolynomialRegression(degree=10)
poly10_reg.fit(X, y)

y10_predict = poly10_reg.predict(X)
mean_squared_error(y, y10_predict)
# 1.0508466763764164
plt.scatter(x, y)
plt.plot(np.sort(x), y10_predict[np.argsort(x)], color='r')
plt.show()

poly100_reg = PolynomialRegression(degree=100)
poly100_reg.fit(X, y)

y100_predict = poly100_reg.predict(X)
mean_squared_error(y, y100_predict)
# 0.68743577834336944
plt.scatter(x, y)
plt.plot(np.sort(x), y100_predict[np.argsort(x)], color='r')
plt.show()

X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
y_plot = poly100_reg.predict(X_plot)
plt.scatter(x, y)
plt.plot(X_plot[:,0], y_plot, color='r')
plt.axis([-3, 3, 0, 10])
plt.show()

## train test split的意义
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
lin_reg.fit(X_train, y_train)
y_predict = lin_reg.predict(X_test)
mean_squared_error(y_test, y_predict)
# 2.2199965269396573
poly2_reg.fit(X_train, y_train)
y2_predict = poly2_reg.predict(X_test)
mean_squared_error(y_test, y2_predict)
# 0.80356410562978997
poly10_reg.fit(X_train, y_train)
y10_predict = poly10_reg.predict(X_test)
mean_squared_error(y_test, y10_predict)
# 0.92129307221507939
poly100_reg.fit(X_train, y_train)
y100_predict = poly100_reg.predict(X_test)
mean_squared_error(y_test, y100_predict)
# 14075796419.234262















#### 8-4 为什么要有训练数据集与测试数据集 (16:09)

#### 8-5 学习曲线 (15:28)
05-Learning-Curve.ipynb 74 KB
  
## 学习曲线
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
plt.scatter(x, y)
plt.show()

## 学习曲线
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)
X_train.shape
# (75, 1)
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

train_score = []
test_score = []
for i in range(1, 76):
    lin_reg = LinearRegression()
    lin_reg.fit(X_train[:i], y_train[:i])
    
    y_train_predict = lin_reg.predict(X_train[:i])
    train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    
    y_test_predict = lin_reg.predict(X_test)
    test_score.append(mean_squared_error(y_test, y_test_predict))
plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
plt.legend()
plt.show()

def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
    train_score = []
    test_score = []
    for i in range(1, len(X_train)+1):
        algo.fit(X_train[:i], y_train[:i])
    
        y_train_predict = algo.predict(X_train[:i])
        train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    
        y_test_predict = algo.predict(X_test)
        test_score.append(mean_squared_error(y_test, y_test_predict))
        
    plt.plot([i for i in range(1, len(X_train)+1)], 
                               np.sqrt(train_score), label="train")
    plt.plot([i for i in range(1, len(X_train)+1)], 
                               np.sqrt(test_score), label="test")
    plt.legend()
    plt.axis([0, len(X_train)+1, 0, 4])
    plt.show()
    
plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test)

from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])

poly2_reg = PolynomialRegression(degree=2)
plot_learning_curve(poly2_reg, X_train, X_test, y_train, y_test)

poly20_reg = PolynomialRegression(degree=20)
plot_learning_curve(poly20_reg, X_train, X_test, y_train, y_test)


#### 8-6 验证数据集与交叉验证 (25:20)
##06-Validation-and-Cross-Validation.ipynb 7.5 KB
  
##Validation 和 Cross Validation
import numpy as np
from sklearn import datasets
digits = datasets.load_digits()
X = digits.data
y = digits.target

## 测试train_test_split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=666)
from sklearn.neighbors import KNeighborsClassifier

best_k, best_p, best_score = 0, 0, 0
for k in range(2, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score > best_score:
            best_k, best_p, best_score = k, p, score
            
print("Best K =", best_k)
print("Best P =", best_p)
print("Best Score =", best_score)
# Best K = 3
# Best P = 4
# Best Score = 0.986091794159
# 使用交叉验证
from sklearn.model_selection import cross_val_score

knn_clf = KNeighborsClassifier()
cross_val_score(knn_clf, X_train, y_train)
array([ 0.98895028,  0.97777778,  0.96629213])
best_k, best_p, best_score = 0, 0, 0
for k in range(2, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        scores = cross_val_score(knn_clf, X_train, y_train)
        score = np.mean(scores)
        if score > best_score:
            best_k, best_p, best_score = k, p, score
            
print("Best K =", best_k)
print("Best P =", best_p)
print("Best Score =", best_score)
# Best K = 2
# Best P = 2
# Best Score = 0.982359987401
best_knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=2, p=2)
best_knn_clf.fit(X_train, y_train)
best_knn_clf.score(X_test, y_test)
# 0.98052851182197498

## 回顾网格搜索
from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        'weights': ['distance'],
        'n_neighbors': [i for i in range(2, 11)], 
        'p': [i for i in range(1, 6)]
    }
]

grid_search = GridSearchCV(knn_clf, param_grid, verbose=1)
grid_search.fit(X_train, y_train)
# Fitting 3 folds for each of 45 candidates, totalling 135 fits
# [Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:  1.9min finished
# GridSearchCV(cv=None, error_score='raise',
#        estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=10, p=5,
#            weights='distance'),
#        fit_params={}, iid=True, n_jobs=1,
#        param_grid=[{'weights': ['distance'], 'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'p': [1, 2, 3, 4, 5]}],
#        pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
#        scoring=None, verbose=1)
grid_search.best_score_
# 0.98237476808905377
grid_search.best_params_
# {'n_neighbors': 2, 'p': 2, 'weights': 'distance'}
best_knn_clf = grid_search.best_estimator_
best_knn_clf.score(X_test, y_test)
# 0.98052851182197498

## cv参数
cross_val_score(knn_clf, X_train, y_train, cv=5)
# array([ 0.99543379,  0.96803653,  0.98148148,  0.96261682,  0.97619048])
grid_search = GridSearchCV(knn_clf, param_grid, verbose=1, cv=5)



#### 8-7 偏差方差平衡 (15:16)
	## 有些算法是天生高方差的，如 kNN、决策树
	## 非参数学习通常都是高方差的，因为对数据不做假设
	## 有些是天生高偏差的，如线性回归（因为对数据有极强的假设）
	# 误差 = 偏差 + 方差 + 不可避免的的误差
	#     偏差： 欠拟合（假设不正确、变量和目标关联性很低，如用学生的名字预测他的成绩）
	#     高偏差：没有学到问题的实质，如学习到了噪音，模型过为复杂（过拟合）
	# 高方差算法

	# 机器学习从算法角度，我们的挑战主要来自于方差
	# 问题的理解过于肤浅，假设我们有比较好的数据，也有比较好的特征，我们关注与算法，基于这些数据得到可靠的结果

	# 解决高方差的主要手段
		# 1. 降低模型复杂度
		# 2. 减少数据维度、降噪
		# 3. 增加样本量（有时算法过于复杂、参数过多，样本量不足以支撑计算出如此多的参数，如，神经网络、深度学习就是最典型的例子）
		# 4. 使用验证集

#### 8-8 模型泛化与岭回归 (19:15)
08-Model-Regularization-and-Ridge-Regression.ipynb 102 KB
  
## 岭回归 Ridge Regression
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(42)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x + 3 + np.random.normal(0, 1, size=100)
plt.scatter(x, y)
plt.show()

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])
from sklearn.model_selection import train_test_split

np.random.seed(666)
X_train, X_test, y_train, y_test = train_test_split(X, y)
from sklearn.metrics import mean_squared_error

poly_reg = PolynomialRegression(degree=20)
poly_reg.fit(X_train, y_train)

y_poly_predict = poly_reg.predict(X_test)
mean_squared_error(y_test, y_poly_predict)
# 167.94010867293571
X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
y_plot = poly_reg.predict(X_plot)

plt.scatter(x, y)
plt.plot(X_plot[:,0], y_plot, color='r')
plt.axis([-3, 3, 0, 6])
plt.show()

def plot_model(model):
    X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
    y_plot = model.predict(X_plot)

    plt.scatter(x, y)
    plt.plot(X_plot[:,0], y_plot, color='r')
    plt.axis([-3, 3, 0, 6])
    plt.show()

plot_model(poly_reg)

## 使用岭回归
from sklearn.linear_model import Ridge

def RidgeRegression(degree, alpha):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("ridge_reg", Ridge(alpha=alpha))
    ])
ridge1_reg = RidgeRegression(20, 0.0001)
ridge1_reg.fit(X_train, y_train)

y1_predict = ridge1_reg.predict(X_test)
mean_squared_error(y_test, y1_predict)
# 1.3233492754051845
plot_model(ridge1_reg)

ridge2_reg = RidgeRegression(20, 1)
ridge2_reg.fit(X_train, y_train)

y2_predict = ridge2_reg.predict(X_test)
mean_squared_error(y_test, y2_predict)
# 1.1888759304218448
plot_model(ridge2_reg)

ridge3_reg = RidgeRegression(20, 100)
ridge3_reg.fit(X_train, y_train)

y3_predict = ridge3_reg.predict(X_test)
mean_squared_error(y_test, y3_predict)
# 1.3196456113086197
plot_model(ridge3_reg)

ridge4_reg = RidgeRegression(20, 10000000)
ridge4_reg.fit(X_train, y_train)

y4_predict = ridge4_reg.predict(X_test)
mean_squared_error(y_test, y4_predict)
# 1.8408455590998372
plot_model(ridge4_reg)


#### 8-9 LASSO (16:59)
09-LASSO-Regression.ipynb 66 KB
  
LASSO
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(42)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x + 3 + np.random.normal(0, 1, size=100)
plt.scatter(x, y)
plt.show()

from sklearn.model_selection import train_test_split

np.random.seed(666)
X_train, X_test, y_train, y_test = train_test_split(X, y)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])
from sklearn.metrics import mean_squared_error

poly_reg = PolynomialRegression(degree=20)
poly_reg.fit(X_train, y_train)

y_predict = poly_reg.predict(X_test)
mean_squared_error(y_test, y_predict)
# 167.94010867293571
def plot_model(model):
    X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
    y_plot = model.predict(X_plot)

    plt.scatter(x, y)
    plt.plot(X_plot[:,0], y_plot, color='r')
    plt.axis([-3, 3, 0, 6])
    plt.show()

plot_model(poly_reg)

from sklearn.linear_model import Lasso

def LassoRegression(degree, alpha):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lasso_reg", Lasso(alpha=alpha))
    ])
lasso1_reg = LassoRegression(20, 0.01)
lasso1_reg.fit(X_train, y_train)

y1_predict = lasso1_reg.predict(X_test)
mean_squared_error(y_test, y1_predict)
# 1.1496080843259966
plot_model(lasso1_reg)

lasso2_reg = LassoRegression(20, 0.1)
lasso2_reg.fit(X_train, y_train)

y2_predict = lasso2_reg.predict(X_test)
mean_squared_error(y_test, y2_predict)
# 1.1213911351818648
plot_model(lasso2_reg)

lasso3_reg = LassoRegression(20, 1)
lasso3_reg.fit(X_train, y_train)

y3_predict = lasso3_reg.predict(X_test)
mean_squared_error(y_test, y3_predict)
# 1.8408939659515595
plot_model(lasso3_reg)

####8-10 L1, L2和弹性网络 (14:25)

########	第9章 逻辑回归
#### 9-0
# 逻辑回归没有数学解，只能用梯度下降法求解
# 但是只有唯一解


#### 9-1 什么是逻辑回归 (16:07)
## 01-What-is-Logistic-Regression.ipynb 12 KB
  
## sigmoid 函数
import numpy as np
import matplotlib.pyplot as plt
def sigmoid(t):
    return 1. / (1. + np.exp(-t))
x = np.linspace(-10, 10, 500)

plt.plot(x, sigmoid(x))
plt.show()


#### 9-2 逻辑回归的损失函数 (15:00)
#### 9-3 逻辑回归损失函数的梯度 (17:50)
#### 9-4 实现逻辑回归算法 (14:29)
#### 9-5 决策边界 (21:09)
05-Decision-Boundary.ipynb 90 KB
  
决策边界
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()

X = iris.data
y = iris.target

X = X[y<2,:2]
y = y[y<2]
plt.scatter(X[y==0,0], X[y==0,1], color="red")
plt.scatter(X[y==1,0], X[y==1,1], color="blue")
plt.show()

from playML.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, seed=666)
from playML.LogisticRegression import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
LogisticRegression()
log_reg.coef_
# array([ 3.01796521, -5.04447145])
log_reg.intercept_
# -0.6937719272911228
def x2(x1):
    return (-log_reg.coef_[0] * x1 - log_reg.intercept_) / log_reg.coef_[1]
x1_plot = np.linspace(4, 8, 1000)
x2_plot = x2(x1_plot)
plt.scatter(X[y==0,0], X[y==0,1], color="red")
plt.scatter(X[y==1,0], X[y==1,1], color="blue")
plt.plot(x1_plot, x2_plot)
plt.show()

plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1], color="red")
plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1], color="blue")
plt.plot(x1_plot, x2_plot)
plt.show()

def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
    
plot_decision_boundary(log_reg, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# kNN的决策边界
from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=5, p=2,
#            weights='uniform')
knn_clf.score(X_test, y_test)
# 1.0
plot_decision_boundary(knn_clf, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

knn_clf_all = KNeighborsClassifier()
knn_clf_all.fit(iris.data[:,:2], iris.target)
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=5, p=2,
#            weights='uniform')
plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])
plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1])
plt.show()

knn_clf_all = KNeighborsClassifier(n_neighbors=50)
knn_clf_all.fit(iris.data[:,:2], iris.target)

plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])
plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1])
plt.show()


#### 9-6 在逻辑回归中使用多项式特征 (15:09)
# 06-Polynomial-Features-in-Logistic-Regression.ipynb 63 KB
  
# 逻辑回归中添加多项式特征
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.normal(0, 1, size=(200, 2))
y = np.array((X[:,0]**2+X[:,1]**2)<1.5, dtype='int')
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# 使用逻辑回归
from playML.LogisticRegression import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(X, y)
LogisticRegression()
log_reg.score(X, y)
# 0.60499999999999998
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

def PolynomialLogisticRegression(degree):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression())
    ])
poly_log_reg = PolynomialLogisticRegression(degree=2)
poly_log_reg.fit(X, y)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression())])
poly_log_reg.score(X, y)
# 0.94999999999999996
plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

poly_log_reg2 = PolynomialLogisticRegression(degree=20)
poly_log_reg2.fit(X, y)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression())])
plot_decision_boundary(poly_log_reg2, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()




#### 9-7 scikit-learn中的逻辑回归 (17:22)
# 07-Logistic-Regression-in-scikit-learn.ipynb 102 KB
  
scikit-learn中的逻辑回归
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.normal(0, 1, size=(200, 2))
y = np.array((X[:,0]**2+X[:,1])<1.5, dtype='int')
for _ in range(20):
    y[np.random.randint(200)] = 1
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
# 使用scikit-learn中的逻辑回归
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
# LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False)
log_reg.score(X_train, y_train)
# 0.79333333333333333
log_reg.score(X_test, y_test)
# 0.85999999999999999
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

def PolynomialLogisticRegression(degree):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression())
    ])
poly_log_reg = PolynomialLogisticRegression(degree=2)
poly_log_reg.fit(X_train, y_train)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False))])
poly_log_reg.score(X_train, y_train)
# 0.91333333333333333
poly_log_reg.score(X_test, y_test)
# 0.93999999999999995
plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

poly_log_reg2 = PolynomialLogisticRegression(degree=20)
poly_log_reg2.fit(X_train, y_train)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False))])
poly_log_reg2.score(X_train, y_train)
# 0.93999999999999995
poly_log_reg2.score(X_test, y_test)
# 0.92000000000000004
plot_decision_boundary(poly_log_reg2, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

def PolynomialLogisticRegression(degree, C):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C=C))
    ])

poly_log_reg3 = PolynomialLogisticRegression(degree=20, C=0.1)
poly_log_reg3.fit(X_train, y_train)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False))])
poly_log_reg3.score(X_train, y_train)
# 0.85333333333333339
poly_log_reg3.score(X_test, y_test)
# 0.92000000000000004
plot_decision_boundary(poly_log_reg3, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

def PolynomialLogisticRegression(degree, C, penalty='l2'):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C=C, penalty=penalty))
    ])

poly_log_reg4 = PolynomialLogisticRegression(degree=20, C=0.1, penalty='l1')
poly_log_reg4.fit(X_train, y_train)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False))])
poly_log_reg4.score(X_train, y_train)
# 0.82666666666666666
poly_log_reg4.score(X_test, y_test)
# 0.90000000000000002
plot_decision_boundary(poly_log_reg4, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()



#### 9-8 OvR与OvO (19:09)
OvR 和 OvO
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:,:2]
y = iris.target
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
# LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False)
log_reg.score(X_test, y_test)
# 0.65789473684210531
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(log_reg, axis=[4, 8.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

log_reg2 = LogisticRegression(multi_class="multinomial", solver="newton-cg")
log_reg2.fit(X_train, y_train)
log_reg2.score(X_test, y_test)
# 0.78947368421052633
plot_decision_boundary(log_reg2, axis=[4, 8.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

# 使用所有的数据
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
# 0.94736842105263153
log_reg2 = LogisticRegression(multi_class="multinomial", solver="newton-cg")
log_reg2.fit(X_train, y_train)
log_reg2.score(X_test, y_test)
# 1.0
# OvO and OvR
from sklearn.multiclass import OneVsRestClassifier

ovr = OneVsRestClassifier(log_reg)
ovr.fit(X_train, y_train)
ovr.score(X_test, y_test)
# 0.94736842105263153
from sklearn.multiclass import OneVsOneClassifier

ovo = OneVsOneClassifier(log_reg)
ovo.fit(X_train, y_train)
ovo.score(X_test, y_test)
# 1.0


########	第10章 评价分类结果
# 极度偏斜的数据 Skewed Data， 需要引入分类准确度以外的其他的指标

10-1 准确度的陷阱和混淆矩阵 (15:06)

10-2 精准率和召回率 (11:51)

10-3 实现混淆矩阵，精准率和召回率 (15:42)
# 03-Implement-Confusion-Matrix-Precision-and-Recall.ipynb 7.0 KB
  
# 实现混淆矩阵，精准率和召回率
import numpy as np
from sklearn import datasets
digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
# 0.97555555555555551
y_log_predict = log_reg.predict(X_test)
def TN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 0))

TN(y_test, y_log_predict)
# 403
def FP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 1))

FP(y_test, y_log_predict)
# 2
def FN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 0))

FN(y_test, y_log_predict)
# 9
def TP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 1))

TP(y_test, y_log_predict)
# 36
def confusion_matrix(y_true, y_predict):
    return np.array([
        [TN(y_true, y_predict), FP(y_true, y_predict)],
        [FN(y_true, y_predict), TP(y_true, y_predict)]
    ])

confusion_matrix(y_test, y_log_predict)
# array([[403,   2],
#        [  9,  36]])
def precision_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fp = FP(y_true, y_predict)
    try:
        return tp / (tp + fp)
    except:
        return 0.0
    
precision_score(y_test, y_log_predict)
# 0.94736842105263153
def recall_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fn = FN(y_true, y_predict)
    try:
        return tp / (tp + fn)
    except:
        return 0.0
    
recall_score(y_test, y_log_predict)
# 0.80000000000000004


##	scikit-learn中的混淆矩阵，精准率和召回率
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_log_predict)
# array([[403,   2],
#        [  9,  36]])
from sklearn.metrics import precision_score

precision_score(y_test, y_log_predict)
# 0.94736842105263153
from sklearn.metrics import recall_score

recall_score(y_test, y_log_predict)
# 0.80000000000000004


####	10-4 F1 Score (17:42)
## F1 Score 是精准率和召回率的调和平均值  
# 2*recall*precision/(recall+precision)
# 1/F1Score = 1/2(1/precision + 1/recall)

# 04-F1-Score.ipynb 5.0 KB
  
# F1 Score
import numpy as np
def f1_score(precision, recall):
    try:
        return 2 * precision * recall / (precision + recall)
    except:
        return 0.0
precision = 0.5
recall = 0.5
f1_score(precision, recall)
# 0.5
precision = 0.1
recall = 0.9
f1_score(precision, recall)
# 0.18000000000000002
precision = 0.0
recall = 1.0
f1_score(precision, recall)
# 0.0
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
# 0.97555555555555551
y_predict = log_reg.predict(X_test)
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict)
# array([[403,   2],
#        [  9,  36]])
from sklearn.metrics import precision_score

precision_score(y_test, y_predict)
# 0.94736842105263153
from sklearn.metrics import recall_score

recall_score(y_test, y_predict)
# 0.80000000000000004
from sklearn.metrics import f1_score

f1_score(y_test, y_predict)
# 0.86746987951807231


####	10-5 精准率和召回率的平衡 (20:18)
## 05-Precision-Recall-Tradeoff.ipynb 7.4 KB
## 一般阈值默认为零
## 没有调整阈值的参数，可以借用log_reg.decision_function(X_test)，查看，设定阈值
	# 比如 decision_scores >= 5
  
# 精准度和召回率的平衡
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_predict = log_reg.predict(X_test)
from sklearn.metrics import f1_score

f1_score(y_test, y_predict)
# 0.86746987951807231
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict)
# array([[403,   2],
#        [  9,  36]])
from sklearn.metrics import precision_score

precision_score(y_test, y_predict)
# 0.94736842105263153
from sklearn.metrics import recall_score

recall_score(y_test, y_predict)
# 0.80000000000000004
log_reg.decision_function(X_test)[:10]
# array([-22.05700185, -33.02943631, -16.21335414, -80.37912074,
#        -48.25121102, -24.54004847, -44.39161228, -25.0429358 ,
#         -0.97827574, -19.71740779])
log_reg.predict(X_test)[:10]
# array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
decision_scores = log_reg.decision_function(X_test)
np.min(decision_scores)
# -85.686124167491727
np.max(decision_scores)
# 19.889606885682948
y_predict_2 = np.array(decision_scores >= 5, dtype='int')
confusion_matrix(y_test, y_predict_2)
# array([[404,   1],
#        [ 21,  24]])
precision_score(y_test, y_predict_2)
# 0.95999999999999996
recall_score(y_test, y_predict_2)
# 0.53333333333333333
y_predict_3 = np.array(decision_scores >= -5, dtype='int')
confusion_matrix(y_test, y_predict_3)
# array([[390,  15],
#        [  5,  40]])
precision_score(y_test, y_predict_3)
# 0.72727272727272729
recall_score(y_test, y_predict_3)
# 0.88888888888888884





####	10-6 精准率-召回率曲线 (14:21)
# 06-Precision-Recall-Curve.ipynb 47 KB
  
# 精准度-召回率曲线
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
decision_scores = log_reg.decision_function(X_test)
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

precisions = []
recalls = []
thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)
for threshold in thresholds:
    y_predict = np.array(decision_scores >= threshold, dtype='int')
    precisions.append(precision_score(y_test, y_predict))
    recalls.append(recall_score(y_test, y_predict))
plt.plot(thresholds, precisions)
plt.plot(thresholds, recalls)
plt.show()

# Precision-Recall 曲线
plt.plot(precisions, recalls)
plt.show()

scikit-learn中的Precision-Recall曲线
from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)
precisions.shape
# (145,)
recalls.shape
# (145,)
thresholds.shape
# (144,)
plt.plot(thresholds, precisions[:-1])
plt.plot(thresholds, recalls[:-1])
plt.show()

plt.plot(precisions, recalls)
plt.show()

10-7 ROC曲线 (16:52)
## Recieve Operation Characterristic Curve
## 应用于比较、选择模型（比较哪个ROC的面积大）
## 描述 TPR（Recall， TP/(TP+FN)） 和 FPR (FP/(TN+FP)) 之间的关系
## TPR和FPR相互间呈现一致的趋势

# ROC 曲线
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
decision_scores = log_reg.decision_function(X_test)
from playML.metrics import FPR, TPR

fprs = []
tprs = []
thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)
for threshold in thresholds:
    y_predict = np.array(decision_scores >= threshold, dtype='int')
    fprs.append(FPR(y_test, y_predict))
    tprs.append(TPR(y_test, y_predict))
plt.plot(fprs, tprs)
plt.show()

# scikit-learn中的ROC
from sklearn.metrics import roc_curve

fprs, tprs, thresholds = roc_curve(y_test, decision_scores)
plt.plot(fprs, tprs)
plt.show()

# ROC AUC （area under curve)
from sklearn.metrics import roc_auc_score

roc_auc_score(y_test, decision_scores)
# 0.98304526748971188


####	10-8 多分类问题中的混淆矩阵 (13:30)
## 08-Confusion-Matrix-in-Multiclass-Classification.ipynb 15 KB
## 参数设定 precision_score(y_test, y_predict, average="micro")
  
# 多分类问题中的混淆矩阵
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
# 0.93115438108484005
y_predict = log_reg.predict(X_test)
from sklearn.metrics import precision_score

precision_score(y_test, y_predict)

precision_score(y_test, y_predict, average="micro")
# 0.93115438108484005
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict)
# array([[147,   0,   1,   0,   0,   1,   0,   0,   0,   0],
#        [  0, 123,   1,   2,   0,   0,   0,   3,   4,  10],
#        [  0,   0, 134,   1,   0,   0,   0,   0,   1,   0],
#        [  0,   0,   0, 138,   0,   5,   0,   1,   5,   0],
#        [  2,   5,   0,   0, 139,   0,   0,   3,   0,   1],
#        [  1,   3,   1,   0,   0, 146,   0,   0,   1,   0],
#        [  0,   2,   0,   0,   0,   1, 131,   0,   2,   0],
#        [  0,   0,   0,   1,   0,   0,   0, 132,   1,   2],
#        [  1,   9,   2,   3,   2,   4,   0,   0, 115,   4],
#        [  0,   1,   0,   5,   0,   3,   0,   2,   2, 134]])
cfm = confusion_matrix(y_test, y_predict)
plt.matshow(cfm, cmap=plt.cm.gray)
plt.show()

row_sums = np.sum(cfm, axis=1)
err_matrix = cfm / row_sums
np.fill_diagonal(err_matrix, 0)

plt.matshow(err_matrix, cmap=plt.cm.gray)
plt.show()


########	第11章 支撑向量机 SVM
####	11-0 
	##	!! 和kNN一样需要做数据标准化（因为涉及距离计算）
	##	support vector machine
	##	SVM 尝试寻找一个最优的决策边界
	##	距离两个类别的最近的样本最远， 而最近的点被称为支撑向量，同时决策边界是由这些支撑向量所决定的
	##	SVM 任务是最大化Margin （典型的转为最优化的问题）
		# 线性可分的情况下 Hard Margin SVM
		# 实际情况下通常是线性不可分的，Soft Margin SVM，能够有一定的容错能力
	##	求解一个有条件的数学最优化问题

####	11-1 什么是SVM (13:54)

####	11-2 SVM背后的最优化问题 (19:21)

####	11-3 Soft Margin SVM (16:12)

####	11-4 scikit-learn中的SVM (21:23)
# 04-SVM-in-scikit-learn.ipynb 69 KB
  
# scikit-learn中的SVM
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()

X = iris.data
y = iris.target

X = X[y<2,:2]
y = y[y<2]
plt.scatter(X[y==0,0], X[y==0,1], color='red')
plt.scatter(X[y==1,0], X[y==1,1], color='blue')
plt.show()

from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X)
X_standard = standardScaler.transform(X)
from sklearn.svm import LinearSVC

svc = LinearSVC(C=1e9)
svc.fit(X_standard, y)
# LinearSVC(C=1000000000.0, class_weight=None, dual=True, fit_intercept=True,
#      intercept_scaling=1, loss='squared_hinge', max_iter=1000,
#      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
#      verbose=0)
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(svc, axis=[-3, 3, -3, 3])
plt.scatter(X_standard[y==0,0], X_standard[y==0,1])
plt.scatter(X_standard[y==1,0], X_standard[y==1,1])
plt.show()

svc2 = LinearSVC(C=0.01)
svc2.fit(X_standard, y)
# LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,
#      intercept_scaling=1, loss='squared_hinge', max_iter=1000,
#      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
#      verbose=0)
plot_decision_boundary(svc2, axis=[-3, 3, -3, 3])
plt.scatter(X_standard[y==0,0], X_standard[y==0,1])
plt.scatter(X_standard[y==1,0], X_standard[y==1,1])
plt.show()

svc.coef_
# array([[ 4.03243305, -2.49295041]])
svc.intercept_
# array([ 0.9536471])
def plot_svc_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
    
    w = model.coef_[0]
    b = model.intercept_[0]
    
    # w0*x0 + w1*x1 + b = 0
    # => x1 = -w0/w1 * x0 - b/w1
    plot_x = np.linspace(axis[0], axis[1], 200)
    up_y = -w[0]/w[1] * plot_x - b/w[1] + 1/w[1]
    down_y = -w[0]/w[1] * plot_x - b/w[1] - 1/w[1]
    
    up_index = (up_y >= axis[2]) & (up_y <= axis[3])
    down_index = (down_y >= axis[2]) & (down_y <= axis[3])
    plt.plot(plot_x[up_index], up_y[up_index], color='black')
    plt.plot(plot_x[down_index], down_y[down_index], color='black')
plot_svc_decision_boundary(svc, axis=[-3, 3, -3, 3])
plt.scatter(X_standard[y==0,0], X_standard[y==0,1])
plt.scatter(X_standard[y==1,0], X_standard[y==1,1])
plt.show()

plot_svc_decision_boundary(svc2, axis=[-3, 3, -3, 3])
plt.scatter(X_standard[y==0,0], X_standard[y==0,1])
plt.scatter(X_standard[y==1,0], X_standard[y==1,1])
plt.show()


####	11-5 SVM中使用多项式特征和核函数 (12:43)
## 05-Polynomial-Features-in-SVM-and-Kernel-Function.ipynb 50 KB
  
# SVM中的使用多项式特征
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons()
X.shape
# (100, 2)
y.shape
# (100,)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

X, y = datasets.make_moons(noise=0.15, random_state=666)

plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# 使用多项式特征的SVM
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

def PolynomialSVC(degree, C=1.0):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("linearSVC", LinearSVC(C=C))
    ])
poly_svc = PolynomialSVC(degree=3)
poly_svc.fit(X, y)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearSVC', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
#      intercept_scaling=1, loss='squared_hinge', max_iter=1000,
#      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
#      verbose=0))])
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(poly_svc, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

##! 使用多项式核函数的SVM
from sklearn.svm import SVC

def PolynomialKernelSVC(degree, C=1.0):
    return Pipeline([
        ("std_scaler", StandardScaler()),
        ("kernelSVC", SVC(kernel="poly", degree=degree, C=C))
    ])
poly_kernel_svc = PolynomialKernelSVC(degree=3)
poly_kernel_svc.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('kernelSVC', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma='auto', kernel='poly',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(poly_kernel_svc, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

####	11-6 到底什么是核函数 (15:31)

####	11-7 RBF核函数 (20:46)
## RBF函数 = 高斯函数 = 径向基函数
## 高斯核函数的本质：将每一个样本点映射到一个无穷维的特征空间
	## （前写章的）多项式特征：依靠升维使原本线性不可分的数据线性可分，高斯核函数本质上也是如此
## 对于每个点都是Landmark m*n的数据映射成了m*m的数据
## 应用：样本远远小于变量（m<n）-> 自安语言处理
## 07-What-is-RBF-Kernel.ipynb 15 KB
  
# 直观理解高斯核函数
import numpy as np
import matplotlib.pyplot as plt
x = np.arange(-4, 5, 1)
x
# array([-4, -3, -2, -1,  0,  1,  2,  3,  4])
y = np.array((x >= -2) & (x <= 2), dtype='int')
y
# array([0, 0, 1, 1, 1, 1, 1, 0, 0])
plt.scatter(x[y==0], [0]*len(x[y==0]))
plt.scatter(x[y==1], [0]*len(x[y==1]))
plt.show()

def gaussian(x, l):
    gamma = 1.0
    return np.exp(-gamma * (x-l)**2)
l1, l2 = -1, 1

X_new = np.empty((len(x), 2))
for i, data in enumerate(x):
    X_new[i, 0] = gaussian(data, l1)
    X_new[i, 1] = gaussian(data, l2)
plt.scatter(X_new[y==0,0], X_new[y==0,1])
plt.scatter(X_new[y==1,0], X_new[y==1,1])
plt.show()

####	11-8 RBF核函数中的gamma (13:39)
## 08-RBF-Kernel-in-scikit-learn.ipynb 76 KB
  
scikit-learn 中的 RBF 核
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(noise=0.15, random_state=666)

plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC

def RBFKernelSVC(gamma):
    return Pipeline([
        ("std_scaler", StandardScaler()),
        ("svc", SVC(kernel="rbf", gamma=gamma))
    ])
svc = RBFKernelSVC(gamma=1)
svc.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=1, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(svc, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

svc_gamma100 = RBFKernelSVC(gamma=100)
svc_gamma100.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=100, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(svc_gamma100, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

svc_gamma10 = RBFKernelSVC(gamma=10)
svc_gamma10.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=10, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(svc_gamma10, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

svc_gamma05 = RBFKernelSVC(gamma=0.5)
svc_gamma05.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(svc_gamma05, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

svc_gamma01 = RBFKernelSVC(gamma=0.1)
svc_gamma01.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(svc_gamma01, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()


####	11-9 SVM思想解决回归问题 (10:46)
## 09-SVM-Regressor.ipynb 2.8 KB
  
# SVM 思想解决回归问题
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.svm import LinearSVR
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def StandardLinearSVR(epsilon=0.1):
    return Pipeline([
        ('std_scaler', StandardScaler()),
        ('linearSVR', LinearSVR(epsilon=epsilon))
    ])
svr = StandardLinearSVR()
svr.fit(X_train, y_train)
Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearSVR', LinearSVR(C=1.0, dual=True, epsilon=0.1, fit_intercept=True,
     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,
     random_state=None, tol=0.0001, verbose=0))])
svr.score(X_test, y_test)
# 0.63618523213237332


########	第12章 决策树
#### 12-0
	## 非参数学习算法
	## 天然地可以解决 分类、回归问题

#### 12-1 什么是决策树 (12:01)
## 01-What-is-Decision-Tree.ipynb 22 KB
  
# 什么是决策树
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:,2:]
y = iris.target
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy", random_state=42)
dt_clf.fit(X, y)
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=42,
            splitter='best')
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, cmap=custom_cmap)
plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

####	12-2 信息熵 (16:30)
## 熵越大，数据的不确定性更高
## 熵越小，数据的不确定性更低
## 02-Entropy.ipynb 16 KB
  
# 信息熵
# 二分类问题的函数
import numpy as np
import matplotlib.pyplot as plt
def entropy(p):
    return -p * np.log(p) - (1-p) * np.log(1-p)
x = np.linspace(0.01, 0.99, 200)
plt.plot(x, entropy(x))
plt.show()

####	12-3 使用信息熵寻找最优划分 (20:20)

####	12-4 基尼系数 (17:41)
# 熵信息计算比基尼系数稍慢
# sklearn默认为基尼系数
# 两者大多数时候没有太大的区别


####	12-5 CART与决策树中的超参数 (15:04)
## 05-CART-and-Decision-Tree-Hyperparameters.ipynb 68 KB
  
# CART 和 决策树的超参数
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(noise=0.25, random_state=666)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier()
dt_clf.fit(X, y)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(dt_clf, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

dt_clf2 = DecisionTreeClassifier(max_depth=2)
dt_clf2.fit(X, y)

plot_decision_boundary(dt_clf2, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

dt_clf3 = DecisionTreeClassifier(min_samples_split=10)
dt_clf3.fit(X, y)

plot_decision_boundary(dt_clf3, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

dt_clf4 = DecisionTreeClassifier(min_samples_leaf=6)
dt_clf4.fit(X, y)

plot_decision_boundary(dt_clf4, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

dt_clf5 = DecisionTreeClassifier(max_leaf_nodes=4)
dt_clf5.fit(X, y)

plot_decision_boundary(dt_clf5, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# SkLearn -> Cart -> Classification And Regression Tree
# 其他 ID3, C4.5, C5.0
# 复杂度：O(logm0)、训练：O(n*m*logm) 
# 容易过拟合（所有非参数学习的特点）-> 需要剪枝、解决过拟合


####	12-6 决策树解决回归问题 (08:15)
#	06-Decision-Tree-Regressor.ipynb 2.8 KB
  
# 决策树解决回归问题
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor

dt_reg = DecisionTreeRegressor()
dt_reg.fit(X_train, y_train)
DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,
           max_leaf_nodes=None, min_impurity_decrease=0.0,
           min_impurity_split=None, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           presort=False, random_state=None, splitter='best')
dt_reg.score(X_test, y_test)
# 0.58605479243964098
dt_reg.score(X_train, y_train)
# 1.0   -> 显示了非常容易过拟合

####	12-7 决策树的局限性 (08:16)
# 横平竖直
# 对个别特殊数据敏感

########	第13章 集成学习和随机森林
####	13-1 什么是集成学习 (16:35)
# kNN
# 逻辑回归
# 决策树
# 神经网络
# 贝叶斯
# -> 投票决策 voting classifer

#	01-What-is-Ensemble-Learning.ipynb 28 KB
  
# 什么是集成学习
# hard voting
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression()
log_clf.fit(X_train, y_train)
log_clf.score(X_test, y_test)
0.86399999999999999
from sklearn.svm import SVC

svm_clf = SVC()
svm_clf.fit(X_train, y_train)
svm_clf.score(X_test, y_test)
0.88800000000000001
from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(random_state=666)
dt_clf.fit(X_train, y_train)
dt_clf.score(X_test, y_test)
0.86399999999999999
y_predict1 = log_clf.predict(X_test)
y_predict2 = svm_clf.predict(X_test)
y_predict3 = dt_clf.predict(X_test)
y_predict = np.array((y_predict1 + y_predict2 + y_predict3) >= 2, dtype='int')
y_predict[:10]
array([1, 0, 0, 1, 1, 1, 0, 0, 0, 0])
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_predict)
0.89600000000000002
使用Voting Classifier
from sklearn.ensemble import VotingClassifier

voting_clf = VotingClassifier(estimators=[
    ('log_clf', LogisticRegression()), 
    ('svm_clf', SVC()),
    ('dt_clf', DecisionTreeClassifier(random_state=666))],
                             voting='hard')
voting_clf.fit(X_train, y_train)
voting_clf.score(X_test, y_test)
0.89600000000000002

####	13-2 Soft Voting Classifier (14:30)
# SVC -> 设置 probability=True
# 算法再多，种类也是有限的（投票者还是太少） -> 随机森林
# 使用 Soft Voting Classifier

voting_clf2 = VotingClassifier(estimators=[
    ('log_clf', LogisticRegression()), 
    ('svm_clf', SVC(probability=True)),
    ('dt_clf', DecisionTreeClassifier(random_state=666))],
                             voting='soft')
voting_clf2.fit(X_train, y_train)
voting_clf2.score(X_test, y_test)
0.91200000000000003

####	13-3 Bagging 和 Pasting (16:52)

#	03-Bagging-and-Pasting.ipynb 26 KB
# 创建更多的子模型！ 集成更多的意见、投票者
# 子模型之间不能一致！ 子模型间要有差异性
	# 每个模型只看样本的一部分（如：500个只看100个）
	# 每个子模型并需要太高的准确率

# Bagging 和 Pasting
# 放回取样（bagging）-> 更常用 统计学中：bootstrap， bootstrap=True：放回取样
# 不放回曲阳（pasting）
# 使用决策树型模型，这种非参数学习的方式，更能产生差异比较大的子模型

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) 
使用 Bagging
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                           n_estimators=500, max_samples=100,
                           bootstrap=True)
bagging_clf.fit(X_train, y_train)
bagging_clf.score(X_test, y_test)
0.91200000000000003
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                           n_estimators=5000, max_samples=100,
                           bootstrap=True)
bagging_clf.fit(X_train, y_train)
bagging_clf.score(X_test, y_test)
0.92000000000000004


###	13-4 oob (Out-of-Bag) 和关于Bagging的更多讨论 (14:40)
#	放回取样导致一部分样本很多可能没有取到
#	平均大约有37%的样本没有取到 out-of-bag
#   不需要测试数据集
#	oob_score_

#	更多探讨
	# 针对特征进行随机采样（Random Subspaces）
	# 即针对样本，有针对性特征（Random Patches）

##	04-OOB-and-More-about-Bagging-Classifier.ipynb 29 KB
  
# oob和更多Bagging相关
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42) 
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# oob
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=100,
                               bootstrap=True, oob_score=True)
bagging_clf.fit(X, y)
BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best'),
         bootstrap=True, bootstrap_features=False, max_features=1.0,
         max_samples=100, n_estimators=500, n_jobs=1, oob_score=True,
         random_state=None, verbose=0, warm_start=False)
bagging_clf.oob_score_
0.91800000000000004
# n_jobs
%%time
bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=100,
                               bootstrap=True, oob_score=True)
bagging_clf.fit(X, y)
CPU times: user 1.81 s, sys: 27.2 ms, total: 1.84 s
Wall time: 2.95 s
%%time
bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=100,
                               bootstrap=True, oob_score=True,
                               n_jobs=-1)
bagging_clf.fit(X, y)
CPU times: user 385 ms, sys: 56.1 ms, total: 441 ms
Wall time: 1.83 s

##	bootstrap_features （特征随机取样）
random_subspaces_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=500,
                               bootstrap=True, oob_score=True,
                               max_features=1, bootstrap_features=True)
random_subspaces_clf.fit(X, y)
random_subspaces_clf.oob_score_
0.83399999999999996
random_patches_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=100,
                               bootstrap=True, oob_score=True,
                               max_features=1, bootstrap_features=True)
random_patches_clf.fit(X, y)
random_patches_clf.oob_score_
0.85799999999999998


####	13-5 随机森林和 Extra-Trees (13:14)
# Bagging 、BaseEsimator（Decision Tree） -> 随机森林

#	05-Random-Forest-and-Extra-Trees.ipynb 29 KB
  
# 随机森林
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=666)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# 随机森林
from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(n_estimators=500, oob_score=True, random_state=666, n_jobs=-1)
rf_clf.fit(X, y)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,
            oob_score=True, random_state=666, verbose=0, warm_start=False)
rf_clf.oob_score_
0.89200000000000002
rf_clf2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, oob_score=True, random_state=666, n_jobs=-1)
rf_clf2.fit(X, y)
rf_clf2.oob_score_
0.90600000000000003
# 随机森林拥有决策树和BaggingClassifier的所有参数：）

# Extra-Trees
from sklearn.ensemble import ExtraTreesClassifier

et_clf = ExtraTreesClassifier(n_estimators=500, bootstrap=True, oob_score=True, random_state=666, n_jobs=-1)
et_clf.fit(X, y)
ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',
           max_depth=None, max_features='auto', max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,
           oob_score=True, random_state=666, verbose=0, warm_start=False)
et_clf.oob_score_
0.89200000000000002

# 集成学习解决回归问题
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor

####	13-6 Ada Boosting 和 Gradient Boosting (15:30)
#	集成多个模型
#	每个模型都在尝试增强（Boosting）整体的效果
#	数据 -> 模型1 -> 结果 -> 模型2 -> 结果 -> 模型 ... -> 结果



##	06-AdaBoost-and-Gradient-Boosting.ipynb 29 KB
# Boosting
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=666) 
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
AdaBoosting
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=2), n_estimators=500)
ada_clf.fit(X_train, y_train)
AdaBoostClassifier(algorithm='SAMME.R',
          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best'),
          learning_rate=1.0, n_estimators=500, random_state=None)
ada_clf.score(X_test, y_test)
0.85599999999999998

##	Gradient Boosting -> 不断修正error值
# Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier

gb_clf = GradientBoostingClassifier(max_depth=2, n_estimators=30)
gb_clf.fit(X_train, y_train)
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=2,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=30,
              presort='auto', random_state=None, subsample=1.0, verbose=0,
              warm_start=False)
gb_clf.score(X_test, y_test)
0.90400000000000003



# Boosting 解决回归问题
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor

####	13-7 Stacking (08:46)

########	第14章 更多机器学习算法
####	14-1 学习scikit-learn文档, 大家加油！ (08:32)
# UserGuide：ttps://scikit-learn.org/stable/user_guide.html
# API接口：https://scikit-learn.org/stable/modules/classes.html

####	14-2 学习完这个课程以后怎样继续深入机器学习的学习？
# 第4章 最基础的分类算法-k近邻算法 kNN
# 	04-0
# 		可以解决分类（天然可解决多分类问题）、同时可以解决回归（KNeighborsRegressor）
#		思想简单、效果强大
# 		采用欧拉距离
# 		KNN是一个不需要训练过程的模型（可以理解数据本就是模型）
#		底层实现不像我们的模拟代码这么简单，KNN本身在预测的过程中是非常耗时的
# 		计算距离，投票作出决策
# 		超参数：在算法运行前，需要决定的参数 （调参，一般就是指 超参数）
# 		模型参数：算法过程中学习的参数
# 		kNN没有模型参数
# 		kNN的“k” 是典型的超参数
# 		kNN缺点
			# 缺点1. 效率低下 （m个样本，n个特征，预测每一个数据需要 O(m*n)  omn的时间复杂度
			# 缺点2. 高度数据相关
			# 缺点3. 预测结果不具有可解释性（距离近不等于本质相近）
			# 缺点3. 维度灾难（解决方法：降维）
#   非参数算法：kNN、决策树

# 04-1 k近邻算法基础 (22:42)

######## 4-2 scikit-learn中的机器学习算法封装 (22:22)
import numpy as np
import matplotlib.pyplot as plt
raw_data_X = [[3.393533211, 2.331273381],
              [3.110073483, 1.781539638],
              [1.343808831, 3.368360954],
              [3.582294042, 4.679179110],
              [2.280362439, 2.866990263],
              [7.423436942, 4.696522875],
              [5.745051997, 3.533989803],
              [9.172168622, 2.511101045],
              [7.792783481, 3.424088941],
              [7.939820817, 0.791637231]
             ]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

#list 转换为 numpyArray
X_train = np.array(raw_data_X)
y_train = np.array(raw_data_y)


plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1],color = "r" )
plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1],color = "b")

x = np.array([8.09,3.36])   							#> array([8.09, 3.36])
x.shape 												#> (2,)

# kNN_scikit-learn ！！！！
from sklearn.neighbors import KNeighborsClassifier
kNN_classifier = KNeighborsClassifier(n_neighbors=6)    # 1.实例化
kNN_classifier.fit(X_train,y_train)						# 2.fit

# 转换x为维度数据
X_predict = x.reshape(1, -1) 							#> array([[8.09, 3.36]])
X_predict.shape 										#> (1, 2)

kNN_classifier.predict(x.reshape(1,-1))   				# 3.predict



x_1 = np.array([[8.09,3.36],[5.79,4.78],[2,10]]) 		# predict list
kNN_classifier.predict(x_1) 

# plot
plt.scatter(X_train[y_train==0,0],X_train[y_train==0,1],color="r")
plt.scatter(X_train[y_train==1,0],X_train[y_train==1,1],color="b")
plt.scatter(x_1[0],x_1[1],color="g")


######### 4-3 训练数据集，测试数据集 (22:46) [train test split]
# 训练和测试数据集的分离
# 通过测试数据直接判断模型的好坏
# 在模型进入正式生产环境中改进模型
# 我们甚至可能需要花一章的时间来阐述这个问题

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
iris.keys()
#> dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])

X = iris.data
y = iris.target

X.shape 		#> (150, 4)
y.shape 		#> (150,)

# Train_test_split
y
# array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
#        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
#        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
shuffled_indexes = np.random.permutation(len(X))
shuffled_indexes
# array([ 30, 145,  33,   6, 102,  36,  46,  42, 149,  64, 138,  34,  21,
#         27,  41,  73,  67, 107, 110,  86,  75,  43,  65, 100,  63,   2,
#        121, 109,  31, 140,  91, 130,  79,  45,  50,  26, 144,  38,  78,
#         58,  29,  87, 116,  82,  37,  98,  59,  74,  99,  92,  14,  90,
#         56, 118,  69,  18,  47,  66,  25,  10,   5, 134,  22,  13, 126,
#         62,  71, 137,  23,  32, 125, 129, 115, 112, 148,  70,  52, 108,
#        146, 114, 128,  88,  35,  97,  80,  76,  96,  77,  48,  49,  19,
#        120,  24, 119,  53,  84,  93,  83, 132,  55,  51,  54,  12,   1,
#         40, 136, 124, 139,   9,  72,  94,  95, 117,  28,   4,  17,   8,
#          3, 106, 135, 113, 111,  57, 147,  68,  20,  11, 104,  81, 131,
#         85,  39, 105,  44, 101, 133,  89,   0,   7, 127, 123,  16, 142,
#         60, 143,  61, 122, 141, 103,  15])
test_ratio = 0.2
test_size = int(len(X) * test_ratio)
test_size

test_indexes = shuffled_indexes[:test_size]
train_indexes = shuffled_indexes[test_size:]
test_indexes
# array([ 30, 145,  33,   6, 102,  36,  46,  42, 149,  64, 138,  34,  21,
#         27,  41,  73,  67, 107, 110,  86,  75,  43,  65, 100,  63,   2,
#        121, 109,  31, 140])

X_train = X[train_indexes]
y_train = y[train_indexes]

X_test = X[test_indexes]
y_test = y[test_indexes]


# sklearn train_test_split
from sklearn.model_selection import train_test_split
train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=666)

# 1.实例化 
from sklearn.neighbors import KNeighborsClassifier 
knn_clf = KNeighborsClassifier(n_neighbors=6)

# 2. fit
knn_clf.fit(X_train,y_train)

# 3.Predict
y_predict = knn_clf.predict(X_test)

# result
y_predict
# array([1, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 2, 1, 0, 2, 2, 2, 1, 0,
#        2, 0, 1, 1, 0, 1, 2, 2])
sum(y_predict==y_test)
sum(y_predict==y_test) / len(X_test)



######## 4-4 分类准确度 (19:20)
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
digits.keys() 								#> dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])
print(digits.DESCR)

X = digits.data 							#> (1797, 64)
y = digits.target 							#> (1797,)
digits.target_names							#> array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

y[:100]
# array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,
#        2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7,
#        7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6,
#        6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4,
#        6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1])

X[:10]
# array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,
#         15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,
#         12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,
#          0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,
#         10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.],
#        [ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,
#          9.,  0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7.,
#         15., 16., 16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,
#          0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16.,
#         16.,  6.,  0.,  0.,  0.,  0.,  0., 11., 16., 10.,  0.,  0.],
#         ...

some_digit = X[666]
some_digit_image = some_digit.reshape(8,8)
plt.imshow(some_digit_image,cmap = matplotlib.cm.binary) 		#> 可视化图片


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier(n_neighbors=3)

knn_clf.fit(X_train,y_train)
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=3, p=2,
#            weights='uniform')

y_predict = knn_clf.predict(X_test)

# 4. Accuracy_score¶
from sklearn.metrics import accuracy_score

# --- .accuracy_score >>>>>>> 得出y_predict后
accuracy_score(y_test,y_predict)     #> 0.9888888888888889

# --- .score >>>>>>> 直接得出 accuracy
knn_clf.score(X_test,y_test) 		#> 0.9888888888888889

# --- 手工计算 accuracy
sum(y_predict==y_test)/len(X_test)




######## 4-5 超参数 (21:36)
# 超参数：在算法运行前，需要决定的参数 （调参，一般就是指 超参数）
# 模型参数：算法过程中学习的参数
# kNN没有模型参数
# kNN的“k” 是典型的超参数
# 寻找好的超参数
	# 领域知识
	# 经验数值
	# 试验搜索



import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train,y_train)
knn_clf.score(X_test,y_test)

#> 0.9888888888888889

## 寻找最好的 K
best_score = 0.0
best_k = -1
for k in range(1,11):
    knn_clf = KNeighborsClassifier(n_neighbors=k)
    knn_clf.fit(X_train,y_train)
    score = knn_clf.score(X_test,y_test)
    if score > best_score:
        best_k = k
        best_score = score

print("best_k = ", best_k)
print("best_score = ", best_score)

# best_k =  4
# best_score =  0.9916666666666667

## 考虑距离与否
best_score = 0.0
best_k = -1
for method in ["uniform","distance"]:
    for k in range(1,11):
        knn_clf = KNeighborsClassifier(n_neighbors=k,weights=method)
        knn_clf.fit(X_train,y_train)
        score = knn_clf.score(X_test,y_test)
        if score > best_score:
            best_k = k
            best_score = score
            best_method = method

print("best_method = ", method)
print("best_k = ", best_k)
print("best_score = ", best_score)

# best_method =  distance
# best_k =  4
# best_score =  0.9916666666666667

## --- 搜索明科夫斯基p
best_p = -1
best_score = 0.0
best_k = -1
for k in range(1,11):
    for p in range(1,6):
        knn_clf = KNeighborsClassifier(n_neighbors=k,weights="distance",p=p)
        knn_clf.fit(X_train,y_train)
        score = knn_clf.score(X_test,y_test)
        if score > best_score:
            best_k = k
            best_score = score
            best_p = p

print("best_p = ", best_p)
print("best_k = ", best_k)
print("best_score = ", best_score)

# best_p =  2
# best_k =  3
# best_score =  0.9888888888888889

######## 4-6 网格搜索与k近邻算法中更多超参数 (17:24)
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train,y_train)
knn_clf.score(X_test,y_test)
#> 0.9888888888888889

#### [Grid Search]
param_grid = [
    {
        "weights":["uniform"],
        "n_neighbors":[i for i in range(1,11)]
    },
    {
        "weights":["distance"],
        "n_neighbors":[i for i in range(1,11)],
        "p": [i for i in range(1,6)]
    }
]
# 1. kNN 实例化
knn_clf = KNeighborsClassifier()

# 2. Grid Search 实例化,引用参数
from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(knn_clf,param_grid, n_jobs=-1,verbose=2)

# 3. Fit (Grid search)
%%time
grid_search.fit(X_train,y_train)

# GridSearchCV(cv=None, error_score=nan,
#              estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,
#                                             metric='minkowski',
#                                             metric_params=None, n_jobs=None,
#                                             n_neighbors=5, p=2,
#                                             weights='uniform'),
#              iid='deprecated', n_jobs=-1,
#              param_grid=[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
#                           'weights': ['uniform']},
#                          {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
#                           'p': [1, 2, 3, 4, 5], 'weights': ['distance']}],
#              pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
#              scoring=None, verbose=2)

#### ---- Grid search 运行参数查看
grid_search.best_estimator_
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=3, p=3,
#            weights='distance')

#### ---- best_score_
grid_search.best_score_
#> 0.9853862212943633

#### ---- best_params_
#> {'n_neighbors': 3, 'p': 3, 'weights': 'distance'}

#### ---- grid_search.best_estimator_实例化 knn_clf
knn_clf = grid_search.best_estimator_

# 4. Predict
knn_clf.predict(X_test)
# array([8, 1, 3, 4, 4, 0, 7, 0, 8, 0, 4, 6, 1, 1, 2, 0, 1, 6, 7, 3, 3, 6,
#        5, 2, 9, 4, 0, 2, 0, 3, 0, 8, 7, 2, 3, 5, 1, 3, 1, 5, 8, 6, 2, 6,
#        3, 1, 3, 0, 0, 4, 9, 9, 2, 8, 7, 0, 5, 4, 0, 9, 5, 5, 8, 7, 4, 2,
#        ...

knn_clf.score(X_test,y_test)
#> 0.9833333333333333


######## 4-7 数据归一化 (15:27)
#### 解决量纲不同的问题，将所有的数据都映射到同一尺度中
import numpy as np
import matplotlib.pyplot as plt
# ---- 最值归一化 Normalization  适用于分布有明显边界的数据，缺点：受outlier影响较大
x = np.random.randint(0, 100, 100) 
(x - np.min(x)) / (np.max(x) - np.min(x))
X = np.random.randint(0, 100, (50, 2))
X = np.array(X, dtype=float)
X[:,0] = (X[:,0] - np.min(X[:,0])) / (np.max(X[:,0]) - np.min(X[:,0]))
X[:,1] = (X[:,1] - np.min(X[:,1])) / (np.max(X[:,1]) - np.min(X[:,1]))

plt.scatter(X[:,0], X[:,1])

np.mean(X[:,0])
np.std(X[:,0])
np.mean(X[:,1])
np.std(X[:,1])

# ---- 均值方差归一化 Standardization  归一到均值为0，方差为1 的分布中
# 适用于分布没有明显边界的数据，有可能存在极端数据值
X2 = np.random.randint(0, 100, (50, 2))
X2 = np.array(X2, dtype=float)
X2[:,0] = (X2[:,0] - np.mean(X2[:,0])) / np.std(X2[:,0])
X2[:,1] = (X2[:,1] - np.mean(X2[:,1])) / np.std(X2[:,1])

plt.scatter(X2[:,0], X2[:,1])

np.mean(X2[:,0]) 	#> -2.2204460492503132e-17
np.std(X2[:,0])		#> 1.0
np.mean(X2[:,1]) 	#> -2.6645352591003756e-17
np.std(X2[:,1]) 	#> 1.0


######## 4-8 scikit-learn中的Scaler (19:24)
#### memo
	# 需要保存训练数据集的方差、均值
import numpy as np
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=666)

#### ---- scikit-learn >>>>>>> StandardScaler
# StandardScaler 1. 实例化
from sklearn.preprocessing import StandardScaler
standardScaler = StandardScaler()

# StandardScaler 2. Fit
standardScaler.fit(X_train)
#> StandardScaler(copy=True, with_mean=True, with_std=True)
print(standardScaler.mean_)
print(standardScaler.scale_)
# [5.83416667 3.0825     3.70916667 1.16916667]
# [0.81019502 0.44076874 1.76295187 0.75429833]

X_train = standardScaler.transform(X_train)
X_train[:2,:]
# array([[-0.90616043,  0.94720873, -1.30982967, -1.28485856],
#        [-1.15301457, -0.18717298, -1.30982967, -1.28485856]])
X_test_standard = standardScaler.transform(X_test)
X_test_standard[:2,:]
# array([[-0.28902506, -0.18717298,  0.44858475,  0.43859746],
#        [-0.04217092, -0.64092567,  0.78892303,  1.63175932]])

#### Scaler后运行分类
from sklearn.neighbors import KNeighborsClassifier
# 1.实例化
knn_clf = KNeighborsClassifier(n_neighbors=3)

# 2. Fit
knn_clf.fit(X_train,y_train)
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=3, p=2,
#            weights='uniform')

# 3. Predict
knn_clf.score(X_test_standard, y_test)
#> 1.0

# 4-9 更多有关k近邻算法的思考 (10:22)
# 参见 4-0


######### 第5章 线性回归法
	# 5-0
	# 解决回归问题
	# 思想简单、实现容易
	# 许多强大的非线性模型的基础（多项式、逻辑回归、SVM）
	# 结果具有很好的解释性
	# 蕴含及其学子中的很多重要思想
	# 距离：不使用绝对值，使用差值的平方，是因为平方更容易使方程式的可导
	# 找到损失函数（loss function）、效用函数（utility） → 机器学习（参数学习）本质（背后的思想）就是最优化损失函数、效用函数 →最优化原理（还有凸优化）
		# 比如说对最短的路径感兴趣、对最小的生成数值感兴趣、希望总价值是最大的
# 5-1 简单线性回归 (18:06)
# 5-2 最小二乘法 (11:27)
#### 5-3 简单线性回归的实现 (14:09)
## byself
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1., 2., 3., 4., 5.])
y = np.array([1., 3., 2., 3., 5.])

plt.scatter(x, y)
plt.axis([0, 6, 0, 6])
plt.show()

x_mean = np.mean(x)
y_mean = np.mean(y)

num = 0.0
d = 0.0
for x_i, y_i in zip(x, y):
    num += (x_i - x_mean) * (y_i - y_mean)
    d += (x_i - x_mean) ** 2

 a = num/d
 b = y_mean - a * x_mean
 y_hat = a * x + b

 plt.scatter(x, y)
plt.plot(x, y_hat, color='r')
plt.axis([0, 6, 0, 6])
plt.show()

x_predict = 6
y_predict = a * x_predict + b
y_predict
## [SK SimpleLinearRegression1] [简单线性回归]

reg1 = SimpleLinearRegression1()
reg1.fit(x, y)
reg1.predict(np.array([x_predict]))


# 5-4 向量化 (12:02)
# 5-5 衡量线性回归法的指标：MSE，RMSE和MAE (22:45)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
print(boston.DESCR)

boston.feature_names
x = boston.data[:,5] # 只使用房间数量这个特征
y = boston.target
plt.scatter(x, y)
plt.show()

x = x[y<50.0]
y = y[y<50.0]

print(x.shape)
print(y.shape)

plt.scatter(x,y)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

mean_squared_error(y_test, y_predict)
mean_absolute_error(y_test, y_predict)

######## 5-6 最好的衡量线性回归法的指标：R Squared (16:28)
# 最重要
# baseline model
from sklearn.metrics import r2_score
r2_score(y_test, y_predict)


#### 5-7 多元线性回归和正规方程解 (15:58)
#### 5-8 实现多元线性回归 (13:08)
#### 5-9 使用scikit-learn解决回归问题 (12:42)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target
X = X[y <50.0]
y = y[y <50.0]

X.shape

# split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2,random_state=666)

# 实例化
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()

# fit
lin_reg.fit(X_train,y_train)
lin_reg.score(X_test,y_test)

# 模型结果
lin_reg.coef_

"""
array([-1.18919477e-01,  3.63991462e-02, -3.56494193e-02,  5.66737830e-02,
       -1.16195486e+01,  3.42022185e+00, -2.31470282e-02, -1.19509560e+00,
        2.59339091e-01, -1.40112724e-02, -8.36521175e-01,  7.92283639e-03,
       -3.81966137e-01])
"""
lin_reg.intercept_
#> 34.161435496246355

## kNN Regressor 解决回归问题
from sklearn.neighbors import KNeighborsRegressor

knn_reg =  KNeighborsRegressor()
knn_reg.fit(X_train,y_train)
knn_reg.score(X_test,y_test)
#> 0.5865412198300899


from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        "weights": ["uniform"],
        "n_neighbors": [i for i in range(1, 11)]
    },
    {
        "weights": ["distance"],
        "n_neighbors": [i for i in range(1, 11)],
        "p": [i for i in range(1,6)]
    }
]

knn_reg = KNeighborsRegressor()
grid_search = GridSearchCV(knn_reg,param_grid,n_jobs=-1)
grid_search.fit(X_train,y_train)
grid_search.best_params_
#> {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}
grid_search.best_estimator_
"""
KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=None, n_neighbors=7, p=1,
                    weights='distance')
"""
grid_search.best_score_ #>0.652216494152461
grid_search.best_estimator_.score(X_test,y_test) #> 0.7160666820548708       


#### 5-10 线性回归的可解释性和更多思考 (11:53)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()

X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)

lin_reg.coef_
"""
array([-1.05574295e-01,  3.52748549e-02, -4.35179251e-02,  4.55405227e-01,
       -1.24268073e+01,  3.75411229e+00, -2.36116881e-02, -1.21088069e+00,
        2.50740082e-01, -1.37702943e-02, -8.38888137e-01,  7.93577159e-03,
       -3.50952134e-01])
"""
np.argsort(lin_reg.coef_)
#> array([ 4,  7, 10, 12,  0,  2,  6,  9, 11,  1,  8,  3,  5], dtype=int64)
boston.feature_names[np.argsort(lin_reg.coef_)]
#> array(['NOX', 'DIS', 'PTRATIO', 'LSTAT', 'CRIM', 'INDUS', 'AGE', 'TAX', 'B', 'ZN', 'RAD', 'CHAS', 'RM'], dtype='<U7')


######### 第6章 梯度下降法
#### 6-0 
	# 不是一个机器学习算法
	# 是一种基于搜索的最优化方法
	# 作用：最小化一个损失函数
	# 梯度上升法：最大化一个效用函数

	# 梯度下降法需要对数据进行归一化处理
	# 梯度下降法的优势
		# -> 比正规方程求解的速度快 （变量越多，差异越大）
		# -> 但是样本量多的话，梯度下降法也非常耗时，由此产生了随机梯度下降法


#### 6-1 什么是梯度下降法 (16:20)
	# 过程有不确定性，但通常可以依然差不多的来到最优值
	# 牺牲一定的精度换取一定的时间


# 并不是所有函数都有唯一的极值点（导数为零）
	# 局部最优解、全部最优解
# 解决方案
	# 多次运行，随机化初始点
	# 梯度下降法的初始点也是一个超参数

#### 6-2 模拟实现梯度下降法 (20:11)
	# eta值 一般情况下可以设置为 0.01

#### 6-3 线性回归中的梯度下降法 (10:56)
np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)

X = x.reshape(-1, 1)
#### 6-4 实现线性回归中的梯度下降法 (14:06)
#### 6-5 梯度下降法的向量化和数据标准化 (22:14)
	# 使用梯度下降法需要对数据进行归一化处理，
	# 设置最大循环次数
	# 比如说只用三分之的样本就可以达到差不多的精度


#### 6-6 随机梯度下降法 (17:34)
	# 梯度下降法的优势
		# -> 比正规方程求解的速度快 （变量越多，差异越大）
		# -> 但是样本量多的话，梯度下降法也非常耗时，由此产生了随机梯度下降法
	# 学习率非常重要

# 6-7 scikit-learn中的随机梯度下降法 (15:40)
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

from sklearn.model_selection import train_test_split
train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=666)


from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X_train)
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)


from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor()
%time sgd_reg.fit(X_train_standard,y_train)
sgd_reg.score(X_test_standard,y_test)



sgd_reg = SGDRegressor(n_iter_no_change=50)     # n_iter_no_change=50 -> 整个样本要浏览多少次
%time sgd_reg.fit(X_train_standard, y_train)
sgd_reg.score(X_test_standard, y_test)
%time sgd_reg.fit(X_train_standard, y_train)
sgd_reg.score(X_test_standard, y_test)

#### 6-8 如何确定梯度计算的准确性？调试梯度下降法 (16:29)
#### 6-9 有关梯度下降法的更多深入讨论 (08:37)
	# 批量梯度下降法
	# 随机梯度下降法 -> SGD
	# 小批量梯度下降法

	# 随机的意义
	# 	跳出局部最优解
	# 	更快的运行速度
	# 	机器学习梁宇很多算法都要使用随机的特点(随机搜索、随机森林)

######## 第7章 PCA与梯度上升法
#### 7-1 什么是PCA (17:45)
#### 7-2 使用梯度上升法求解PCA问题 (09:10)
#### 7-3 求数据的主成分PCA (20:09)
#### 7-4 求数据的前n个主成分 (17:36)
#### 7-5 高维数据映射为低维数据 (19:29)
import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100, 2))
X[:,0] = np.random.uniform(0., 100., size=100)
X[:,1] = 0.75 * X[:,0] + 3. + np.random.normal(0, 10., size=100)

from playML.PCA import PCA

pca = PCA(n_components=2)
pca.fit(X)
# PCA(n_components=2)
pca.components_
# array([[ 0.77441964,  0.63267228],
#        [-0.63266853,  0.77442271]])
pca = PCA(n_components=1)
pca.fit(X)
# PCA(n_components=1)
X_reduction = pca.transform(X)
X_reduction.shape
# (100, 1)
X_restore = pca.inverse_transform(X_reduction)
X_restore.shape
# (100, 2)
plt.scatter(X[:,0], X[:,1], color='b', alpha=0.5)
plt.scatter(X_restore[:,0], X_restore[:,1], color='r', alpha=0.5)
plt.show()

## sklearn
from sklearn.decomposition import PCA
pca = PCA(n_components=1)
pca.fit(X)
# PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,
#     svd_solver='auto', tol=0.0, whiten=False)
X_reduction = pca.transform(X)
X_reduction.shape
# (100, 1)

pca.components_
# array([[-0.77441967, -0.63267225]])

X_restore = pca.inverse_transform(X_reduction)
X_restore.shape
# (100, 2)

plt.scatter(X[:,0], X[:,1], color='b', alpha=0.5)
plt.scatter(X_restore[:,0], X_restore[:,1], color='r', alpha=0.5)
plt.show()

# 7-6 scikit-learn中的PCA (18:57)
import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100, 2))
X[:,0] = np.random.uniform(0., 100., size=100)
X[:,1] = 0.75 * X[:,0] + 3. + np.random.normal(0, 10., size=100)

plt.scatter(X[:,0],X[:,1])

from sklearn.decomposition import PCA
pca = PCA(n_components=1)
pca.fit(X)
# PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,
#   svd_solver='auto', tol=0.0, whiten=False)

pca.components_
# array([[0.7686356 , 0.63968689]])

X_reduction = pca.transform(X)
X_reduction.shape
# (100, 1)

X_restore = pca.inverse_transform(X_reduction)
X_restore.shape
# (100, 2)


######### datasets.digits -> KNN 
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=666)

X_train.shape
# (1347, 64)

## kNN 不使用PCA
%%time

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train,y_train)
Wall time: 207 ms
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,
#                      weights='uniform')

knn_clf.score(X_test,y_test)
# 0.9866666666666667

## PCA降维
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(X_train)
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)

%%time
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_reduction,y_train)
# Wall time: 0 ns
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,
#                      weights='uniform')

pca.explained_variance_ratio_
# array([0.14566817, 0.13735469])

pca = PCA(n_components=X_train.shape[1])
pca.fit(X_train)
pca.explained_variance_ratio_
# array([1.45668166e-01, 1.37354688e-01, 1.17777287e-01, 8.49968861e-02,
#        5.86018996e-02, 5.11542945e-02, 4.26605279e-02, 3.60119663e-02,
#        3.41105814e-02, 3.05407804e-02, 2.42337671e-02, 2.28700570e-02,
#        1.80304649e-02, 1.79346003e-02, 1.45798298e-02, 1.42044841e-02,
#        1.29961033e-02, 1.26617002e-02, 1.01728635e-02, 9.09314698e-03,
#        8.85220461e-03, 7.73828332e-03, 7.60516219e-03, 7.11864860e-03,
#        6.85977267e-03, 5.76411920e-03, 5.71688020e-03, 5.08255707e-03,
#        4.89020776e-03, 4.34888085e-03, 3.72917505e-03, 3.57755036e-03,
#        3.26989470e-03, 3.14917937e-03, 3.09269839e-03, 2.87619649e-03,
#        2.50362666e-03, 2.25417403e-03, 2.20030857e-03, 1.98028746e-03,
#        1.88195578e-03, 1.52769283e-03, 1.42823692e-03, 1.38003340e-03,
#        1.17572392e-03, 1.07377463e-03, 9.55152460e-04, 9.00017642e-04,
#        5.79162563e-04, 3.82793717e-04, 2.38328586e-04, 8.40132221e-05,
#        5.60545588e-05, 5.48538930e-05, 1.08077650e-05, 4.01354717e-06,
#        1.23186515e-06, 1.05783059e-06, 6.06659094e-07, 5.86686040e-07,
#        1.71368535e-33, 7.44075955e-34, 7.44075955e-34, 7.15189459e-34])

plt.plot([i for i in range(X_train.shape[1])], 
         [np.sum(pca.explained_variance_ratio_[:i+1]) for i in range(X_train.shape[1])])

pca = PCA(0.95)
pca.fit(X_train)
# PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,
#   svd_solver='auto', tol=0.0, whiten=False)
pca.n_components_
# 28
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)

%%time
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_reduction,y_train)

knn_clf.score(X_test_reduction,y_test)
# 0.98

## 使用PCA对数据进行降维可视化
pca = PCA(n_components=2)
pca.fit(X)
X_reduction = pca.transform(X)

for i in range(10):
    plt.scatter(X_reduction[y==i,0], X_reduction[y==i,1], alpha=0.8)
plt.show()

plt.scatter(X_reduction[y==0,0],X_reduction[y==0,1],alpha=0.3)
plt.scatter(X_reduction[y==1,0],X_reduction[y==1,1],alpha=0.3)
plt.scatter(X_reduction[y==8,0],X_reduction[y==8,1],alpha=0.3)

for i in range(10):
    plt.scatter(X_reduction[y==i,0],X_reduction[y==i,1],alpha=0.8)


#### 7-7 试手MNIST数据集 (12:06)
import numpy as np 

# from sklearn.datasets import fetch_mldata
# mnist = fetch_mldata('MNIST original')
# 在最新版的 sklearn 中，fetch_mldata 被弃用，改为使用 fetch_openml 获得 MNIST 数据集
# 具体见如下代码，后续代码无需改变

from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784')
mnist

X, y = mnist['data'], mnist['target']
X_train = np.array(X[:60000], dtype=float)
y_train = np.array(y[:60000], dtype=float)
X_test = np.array(X[60000:], dtype=float)
y_test = np.array(y[60000:], dtype=float)

X_train.shape
# (60000, 784)
y_train.shape
# (60000,)
X_test.shape
# (10000, 784)
y_test.shape
# (10000,)

## 使用kNN

from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
%time knn_clf.fit(X_train, y_train)

%time knn_clf.score(X_test, y_test)
CPU times: user 11min 5s, sys: 445 ms, total: 11min 5s
Wall time: 11min 6s
# 0.9688

## PCA进行降维
from sklearn.decomposition import PCA 
pca = PCA(0.90)
pca.fit(X_train)
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)
X_train_reduction.shape
# (60000, 87)

knn_clf = KNeighborsClassifier()
%time knn_clf.fit(X_train_reduction, y_train)
# CPU times: user 1.21 s, sys: 12.9 ms, total: 1.22 s
# Wall time: 387 ms
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,
#                      weights='uniform')
%time knn_clf.score(X_test_reduction, y_test)
# CPU times: user 1min 8s, sys: 280 ms, total: 1min 9s
# Wall time: 1min 9s
# 0.9728
# 降维去除了噪音，有可能准确率更高！

# CPU times: user 12.1 s, sys: 123 ms, total: 12.2 s
# Wall time: 12.2 s
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,
#                      weights='uniform')

# 7-8 关于 MNIST 数据集的最新获得方式
# 7-9 使用PCA对数据进行降噪 (10:00)
#### 7-10 人脸识别与特征脸 (13:57)
## 特征脸
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people()
faces.keys()
# dict_keys(['data', 'images', 'target', 'target_names', 'DESCR'])
faces.data.shape
# (13233, 2914)
faces.target_names
# array(['AJ Cook', 'AJ Lamas', 'Aaron Eckhart', ..., 'Zumrati Juma',
#        'Zurab Tsereteli', 'Zydrunas Ilgauskas'], 
#       dtype='<U35')
faces.images.shape
# (13233, 62, 47)
random_indexes = np.random.permutation(len(faces.data))
X = faces.data[random_indexes]
example_faces = X[:36,:]
example_faces.shape
# (36, 2914)

# 特征脸
%%time
from sklearn.decomposition import PCA 
pca = PCA(svd_solver='randomized')
pca.fit(X)
# CPU times: user 3min 24s, sys: 8.6 s, total: 3min 33s
# Wall time: 2min 2s
pca.components_.shape
# (2914, 2914)
plot_faces(pca.components_[:36,:])
def plot_faces(faces):
    
    fig, axes = plt.subplots(6, 6, figsize=(10, 10),
                         subplot_kw={'xticks':[], 'yticks':[]},
    gridspec_kw=dict(hspace=0.1, wspace=0.1)) 
    for i, ax in enumerate(axes.flat):
        ax.imshow(faces[i].reshape(62, 47), cmap='bone')
    plt.show()
    
plot_faces(example_faces)

#更多关于lfw_people数据集
faces2 = fetch_lfw_people(min_faces_per_person=60)
faces2.data.shape
# (1348, 2914)
faces2.target_names
# array(['Ariel Sharon', 'Colin Powell', 'Donald Rumsfeld', 'George W Bush',
#        'Gerhard Schroeder', 'Hugo Chavez', 'Junichiro Koizumi',
#        'Tony Blair'], 
#       dtype='<U17')
len(faces2.target_names)
# 8




########	第8章 多项式回归与模型泛化
#### 8-1 什么是多项式回归 (11:38)
# 什么是多项式回归
import numpy as np 
import matplotlib.pyplot as plt
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, 100)
plt.scatter(x, y)
plt.show()

# 线性回归？
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_predict = lin_reg.predict(X)
plt.scatter(x, y)
plt.plot(x, y_predict, color='r')
plt.show()

# 解决方案， 添加一个特征
X2 = np.hstack([X, X**2])
X2.shape
# (100, 2)
lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()

lin_reg2.coef_
# array([ 0.99870163,  0.54939125])
lin_reg2.intercept_
# 1.8855236786516001


#### 8-2 scikit-learn中的多项式回归与Pipeline (16:26)
scikit-learn中的多项式回归和Pipeline
import numpy as np 
import matplotlib.pyplot as plt
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, 100)

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
poly.fit(X)
X2 = poly.transform(X)
X2.shape
# (100, 3)
X[:5,:]
# array([[ 0.14960154],
#        [ 0.49319423],
#        [-0.87176575],
#        [-1.33024477],
#        [ 0.47383199]])
X2[:5,:]
# array([[ 1.        ,  0.14960154,  0.02238062],
#        [ 1.        ,  0.49319423,  0.24324055],
#        [ 1.        , -0.87176575,  0.75997552],
#        [ 1.        , -1.33024477,  1.76955114],
#        [ 1.        ,  0.47383199,  0.22451675]])

from sklearn.linear_model import LinearRegression

lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()

lin_reg2.coef_
# array([ 0.        ,  0.9460157 ,  0.50420543])
lin_reg2.intercept_
# 2.1536054095953823
## 关于PolynomialFeatures
X = np.arange(1, 11).reshape(-1, 2)
X
# array([[ 1,  2],
#        [ 3,  4],
#        [ 5,  6],
#        [ 7,  8],
#        [ 9, 10]])
poly = PolynomialFeatures(degree=2)
poly.fit(X)
X2 = poly.transform(X)
X2.shape
# (5, 6)
X2
# array([[   1.,    1.,    2.,    1.,    2.,    4.],
#        [   1.,    3.,    4.,    9.,   12.,   16.],
#        [   1.,    5.,    6.,   25.,   30.,   36.],
#        [   1.,    7.,    8.,   49.,   56.,   64.],
#        [   1.,    9.,   10.,   81.,   90.,  100.]])


## Pipeline
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, 100)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

poly_reg = Pipeline([
    ("poly", PolynomialFeatures(degree=2)),
    ("std_scaler", StandardScaler()),
    ("lin_reg", LinearRegression())
])
poly_reg.fit(X, y)
y_predict = poly_reg.predict(X)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()


#### 8-3 过拟合与欠拟合 (14:22)
## 03-Overfitting-and-Underfitting.ipynb 103 KB
## 过拟合和欠拟合
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
plt.scatter(x, y)
plt.show()

## 使用线性回归
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.score(X, y)
# 0.49537078118650091
y_predict = lin_reg.predict(X)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()

from sklearn.metrics import mean_squared_error

y_predict = lin_reg.predict(X)
mean_squared_error(y, y_predict)
# 3.0750025765636577

## 使用多项式回归
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])
poly2_reg = PolynomialRegression(degree=2)
poly2_reg.fit(X, y)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lin_reg', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])
y2_predict = poly2_reg.predict(X)
mean_squared_error(y, y2_predict)
# 1.0987392142417856

plt.scatter(x, y)
plt.plot(np.sort(x), y2_predict[np.argsort(x)], color='r')
plt.show()


poly10_reg = PolynomialRegression(degree=10)
poly10_reg.fit(X, y)

y10_predict = poly10_reg.predict(X)
mean_squared_error(y, y10_predict)
# 1.0508466763764164
plt.scatter(x, y)
plt.plot(np.sort(x), y10_predict[np.argsort(x)], color='r')
plt.show()

poly100_reg = PolynomialRegression(degree=100)
poly100_reg.fit(X, y)

y100_predict = poly100_reg.predict(X)
mean_squared_error(y, y100_predict)
# 0.68743577834336944
plt.scatter(x, y)
plt.plot(np.sort(x), y100_predict[np.argsort(x)], color='r')
plt.show()

X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
y_plot = poly100_reg.predict(X_plot)
plt.scatter(x, y)
plt.plot(X_plot[:,0], y_plot, color='r')
plt.axis([-3, 3, 0, 10])
plt.show()

## train test split的意义
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
lin_reg.fit(X_train, y_train)
y_predict = lin_reg.predict(X_test)
mean_squared_error(y_test, y_predict)
# 2.2199965269396573
poly2_reg.fit(X_train, y_train)
y2_predict = poly2_reg.predict(X_test)
mean_squared_error(y_test, y2_predict)
# 0.80356410562978997
poly10_reg.fit(X_train, y_train)
y10_predict = poly10_reg.predict(X_test)
mean_squared_error(y_test, y10_predict)
# 0.92129307221507939
poly100_reg.fit(X_train, y_train)
y100_predict = poly100_reg.predict(X_test)
mean_squared_error(y_test, y100_predict)
# 14075796419.234262















#### 8-4 为什么要有训练数据集与测试数据集 (16:09)

#### 8-5 学习曲线 (15:28)
05-Learning-Curve.ipynb 74 KB
  
## 学习曲线
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
plt.scatter(x, y)
plt.show()

## 学习曲线
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)
X_train.shape
# (75, 1)
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

train_score = []
test_score = []
for i in range(1, 76):
    lin_reg = LinearRegression()
    lin_reg.fit(X_train[:i], y_train[:i])
    
    y_train_predict = lin_reg.predict(X_train[:i])
    train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    
    y_test_predict = lin_reg.predict(X_test)
    test_score.append(mean_squared_error(y_test, y_test_predict))
plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
plt.legend()
plt.show()

def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
    train_score = []
    test_score = []
    for i in range(1, len(X_train)+1):
        algo.fit(X_train[:i], y_train[:i])
    
        y_train_predict = algo.predict(X_train[:i])
        train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    
        y_test_predict = algo.predict(X_test)
        test_score.append(mean_squared_error(y_test, y_test_predict))
        
    plt.plot([i for i in range(1, len(X_train)+1)], 
                               np.sqrt(train_score), label="train")
    plt.plot([i for i in range(1, len(X_train)+1)], 
                               np.sqrt(test_score), label="test")
    plt.legend()
    plt.axis([0, len(X_train)+1, 0, 4])
    plt.show()
    
plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test)

from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])

poly2_reg = PolynomialRegression(degree=2)
plot_learning_curve(poly2_reg, X_train, X_test, y_train, y_test)

poly20_reg = PolynomialRegression(degree=20)
plot_learning_curve(poly20_reg, X_train, X_test, y_train, y_test)


#### 8-6 验证数据集与交叉验证 (25:20)
##06-Validation-and-Cross-Validation.ipynb 7.5 KB
  
##Validation 和 Cross Validation
import numpy as np
from sklearn import datasets
digits = datasets.load_digits()
X = digits.data
y = digits.target

## 测试train_test_split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=666)
from sklearn.neighbors import KNeighborsClassifier

best_k, best_p, best_score = 0, 0, 0
for k in range(2, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score > best_score:
            best_k, best_p, best_score = k, p, score
            
print("Best K =", best_k)
print("Best P =", best_p)
print("Best Score =", best_score)
# Best K = 3
# Best P = 4
# Best Score = 0.986091794159
# 使用交叉验证
from sklearn.model_selection import cross_val_score

knn_clf = KNeighborsClassifier()
cross_val_score(knn_clf, X_train, y_train)
array([ 0.98895028,  0.97777778,  0.96629213])
best_k, best_p, best_score = 0, 0, 0
for k in range(2, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        scores = cross_val_score(knn_clf, X_train, y_train)
        score = np.mean(scores)
        if score > best_score:
            best_k, best_p, best_score = k, p, score
            
print("Best K =", best_k)
print("Best P =", best_p)
print("Best Score =", best_score)
# Best K = 2
# Best P = 2
# Best Score = 0.982359987401
best_knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=2, p=2)
best_knn_clf.fit(X_train, y_train)
best_knn_clf.score(X_test, y_test)
# 0.98052851182197498

## 回顾网格搜索
from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        'weights': ['distance'],
        'n_neighbors': [i for i in range(2, 11)], 
        'p': [i for i in range(1, 6)]
    }
]

grid_search = GridSearchCV(knn_clf, param_grid, verbose=1)
grid_search.fit(X_train, y_train)
# Fitting 3 folds for each of 45 candidates, totalling 135 fits
# [Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:  1.9min finished
# GridSearchCV(cv=None, error_score='raise',
#        estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=10, p=5,
#            weights='distance'),
#        fit_params={}, iid=True, n_jobs=1,
#        param_grid=[{'weights': ['distance'], 'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'p': [1, 2, 3, 4, 5]}],
#        pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
#        scoring=None, verbose=1)
grid_search.best_score_
# 0.98237476808905377
grid_search.best_params_
# {'n_neighbors': 2, 'p': 2, 'weights': 'distance'}
best_knn_clf = grid_search.best_estimator_
best_knn_clf.score(X_test, y_test)
# 0.98052851182197498

## cv参数
cross_val_score(knn_clf, X_train, y_train, cv=5)
# array([ 0.99543379,  0.96803653,  0.98148148,  0.96261682,  0.97619048])
grid_search = GridSearchCV(knn_clf, param_grid, verbose=1, cv=5)



#### 8-7 偏差方差平衡 (15:16)
	## 有些算法是天生高方差的，如 kNN、决策树
	## 非参数学习通常都是高方差的，因为对数据不做假设
	## 有些是天生高偏差的，如线性回归（因为对数据有极强的假设）
	# 误差 = 偏差 + 方差 + 不可避免的的误差
	#     偏差： 欠拟合（假设不正确、变量和目标关联性很低，如用学生的名字预测他的成绩）
	#     高偏差：没有学到问题的实质，如学习到了噪音，模型过为复杂（过拟合）
	# 高方差算法

	# 机器学习从算法角度，我们的挑战主要来自于方差
	# 问题的理解过于肤浅，假设我们有比较好的数据，也有比较好的特征，我们关注与算法，基于这些数据得到可靠的结果

	# 解决高方差的主要手段
		# 1. 降低模型复杂度
		# 2. 减少数据维度、降噪
		# 3. 增加样本量（有时算法过于复杂、参数过多，样本量不足以支撑计算出如此多的参数，如，神经网络、深度学习就是最典型的例子）
		# 4. 使用验证集

#### 8-8 模型泛化与岭回归 (19:15)
08-Model-Regularization-and-Ridge-Regression.ipynb 102 KB
  
## 岭回归 Ridge Regression
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(42)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x + 3 + np.random.normal(0, 1, size=100)
plt.scatter(x, y)
plt.show()

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])
from sklearn.model_selection import train_test_split

np.random.seed(666)
X_train, X_test, y_train, y_test = train_test_split(X, y)
from sklearn.metrics import mean_squared_error

poly_reg = PolynomialRegression(degree=20)
poly_reg.fit(X_train, y_train)

y_poly_predict = poly_reg.predict(X_test)
mean_squared_error(y_test, y_poly_predict)
# 167.94010867293571
X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
y_plot = poly_reg.predict(X_plot)

plt.scatter(x, y)
plt.plot(X_plot[:,0], y_plot, color='r')
plt.axis([-3, 3, 0, 6])
plt.show()

def plot_model(model):
    X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
    y_plot = model.predict(X_plot)

    plt.scatter(x, y)
    plt.plot(X_plot[:,0], y_plot, color='r')
    plt.axis([-3, 3, 0, 6])
    plt.show()

plot_model(poly_reg)

## 使用岭回归
from sklearn.linear_model import Ridge

def RidgeRegression(degree, alpha):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("ridge_reg", Ridge(alpha=alpha))
    ])
ridge1_reg = RidgeRegression(20, 0.0001)
ridge1_reg.fit(X_train, y_train)

y1_predict = ridge1_reg.predict(X_test)
mean_squared_error(y_test, y1_predict)
# 1.3233492754051845
plot_model(ridge1_reg)

ridge2_reg = RidgeRegression(20, 1)
ridge2_reg.fit(X_train, y_train)

y2_predict = ridge2_reg.predict(X_test)
mean_squared_error(y_test, y2_predict)
# 1.1888759304218448
plot_model(ridge2_reg)

ridge3_reg = RidgeRegression(20, 100)
ridge3_reg.fit(X_train, y_train)

y3_predict = ridge3_reg.predict(X_test)
mean_squared_error(y_test, y3_predict)
# 1.3196456113086197
plot_model(ridge3_reg)

ridge4_reg = RidgeRegression(20, 10000000)
ridge4_reg.fit(X_train, y_train)

y4_predict = ridge4_reg.predict(X_test)
mean_squared_error(y_test, y4_predict)
# 1.8408455590998372
plot_model(ridge4_reg)


#### 8-9 LASSO (16:59)
09-LASSO-Regression.ipynb 66 KB
  
LASSO
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(42)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x + 3 + np.random.normal(0, 1, size=100)
plt.scatter(x, y)
plt.show()

from sklearn.model_selection import train_test_split

np.random.seed(666)
X_train, X_test, y_train, y_test = train_test_split(X, y)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])
from sklearn.metrics import mean_squared_error

poly_reg = PolynomialRegression(degree=20)
poly_reg.fit(X_train, y_train)

y_predict = poly_reg.predict(X_test)
mean_squared_error(y_test, y_predict)
# 167.94010867293571
def plot_model(model):
    X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
    y_plot = model.predict(X_plot)

    plt.scatter(x, y)
    plt.plot(X_plot[:,0], y_plot, color='r')
    plt.axis([-3, 3, 0, 6])
    plt.show()

plot_model(poly_reg)

from sklearn.linear_model import Lasso

def LassoRegression(degree, alpha):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lasso_reg", Lasso(alpha=alpha))
    ])
lasso1_reg = LassoRegression(20, 0.01)
lasso1_reg.fit(X_train, y_train)

y1_predict = lasso1_reg.predict(X_test)
mean_squared_error(y_test, y1_predict)
# 1.1496080843259966
plot_model(lasso1_reg)

lasso2_reg = LassoRegression(20, 0.1)
lasso2_reg.fit(X_train, y_train)

y2_predict = lasso2_reg.predict(X_test)
mean_squared_error(y_test, y2_predict)
# 1.1213911351818648
plot_model(lasso2_reg)

lasso3_reg = LassoRegression(20, 1)
lasso3_reg.fit(X_train, y_train)

y3_predict = lasso3_reg.predict(X_test)
mean_squared_error(y_test, y3_predict)
# 1.8408939659515595
plot_model(lasso3_reg)

####8-10 L1, L2和弹性网络 (14:25)

########	第9章 逻辑回归
#### 9-0
# 逻辑回归没有数学解，只能用梯度下降法求解
# 但是只有唯一解


#### 9-1 什么是逻辑回归 (16:07)
## 01-What-is-Logistic-Regression.ipynb 12 KB
  
## sigmoid 函数
import numpy as np
import matplotlib.pyplot as plt
def sigmoid(t):
    return 1. / (1. + np.exp(-t))
x = np.linspace(-10, 10, 500)

plt.plot(x, sigmoid(x))
plt.show()


#### 9-2 逻辑回归的损失函数 (15:00)
#### 9-3 逻辑回归损失函数的梯度 (17:50)
#### 9-4 实现逻辑回归算法 (14:29)
#### 9-5 决策边界 (21:09)
05-Decision-Boundary.ipynb 90 KB
  
决策边界
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()

X = iris.data
y = iris.target

X = X[y<2,:2]
y = y[y<2]
plt.scatter(X[y==0,0], X[y==0,1], color="red")
plt.scatter(X[y==1,0], X[y==1,1], color="blue")
plt.show()

from playML.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, seed=666)
from playML.LogisticRegression import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
LogisticRegression()
log_reg.coef_
# array([ 3.01796521, -5.04447145])
log_reg.intercept_
# -0.6937719272911228
def x2(x1):
    return (-log_reg.coef_[0] * x1 - log_reg.intercept_) / log_reg.coef_[1]
x1_plot = np.linspace(4, 8, 1000)
x2_plot = x2(x1_plot)
plt.scatter(X[y==0,0], X[y==0,1], color="red")
plt.scatter(X[y==1,0], X[y==1,1], color="blue")
plt.plot(x1_plot, x2_plot)
plt.show()

plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1], color="red")
plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1], color="blue")
plt.plot(x1_plot, x2_plot)
plt.show()

def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
    
plot_decision_boundary(log_reg, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# kNN的决策边界
from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=5, p=2,
#            weights='uniform')
knn_clf.score(X_test, y_test)
# 1.0
plot_decision_boundary(knn_clf, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

knn_clf_all = KNeighborsClassifier()
knn_clf_all.fit(iris.data[:,:2], iris.target)
# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
#            metric_params=None, n_jobs=1, n_neighbors=5, p=2,
#            weights='uniform')
plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])
plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1])
plt.show()

knn_clf_all = KNeighborsClassifier(n_neighbors=50)
knn_clf_all.fit(iris.data[:,:2], iris.target)

plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])
plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1])
plt.show()


#### 9-6 在逻辑回归中使用多项式特征 (15:09)
# 06-Polynomial-Features-in-Logistic-Regression.ipynb 63 KB
  
# 逻辑回归中添加多项式特征
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.normal(0, 1, size=(200, 2))
y = np.array((X[:,0]**2+X[:,1]**2)<1.5, dtype='int')
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# 使用逻辑回归
from playML.LogisticRegression import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(X, y)
LogisticRegression()
log_reg.score(X, y)
# 0.60499999999999998
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

def PolynomialLogisticRegression(degree):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression())
    ])
poly_log_reg = PolynomialLogisticRegression(degree=2)
poly_log_reg.fit(X, y)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression())])
poly_log_reg.score(X, y)
# 0.94999999999999996
plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

poly_log_reg2 = PolynomialLogisticRegression(degree=20)
poly_log_reg2.fit(X, y)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression())])
plot_decision_boundary(poly_log_reg2, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()




#### 9-7 scikit-learn中的逻辑回归 (17:22)
# 07-Logistic-Regression-in-scikit-learn.ipynb 102 KB
  
scikit-learn中的逻辑回归
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.normal(0, 1, size=(200, 2))
y = np.array((X[:,0]**2+X[:,1])<1.5, dtype='int')
for _ in range(20):
    y[np.random.randint(200)] = 1
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
# 使用scikit-learn中的逻辑回归
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
# LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False)
log_reg.score(X_train, y_train)
# 0.79333333333333333
log_reg.score(X_test, y_test)
# 0.85999999999999999
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

def PolynomialLogisticRegression(degree):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression())
    ])
poly_log_reg = PolynomialLogisticRegression(degree=2)
poly_log_reg.fit(X_train, y_train)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False))])
poly_log_reg.score(X_train, y_train)
# 0.91333333333333333
poly_log_reg.score(X_test, y_test)
# 0.93999999999999995
plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

poly_log_reg2 = PolynomialLogisticRegression(degree=20)
poly_log_reg2.fit(X_train, y_train)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False))])
poly_log_reg2.score(X_train, y_train)
# 0.93999999999999995
poly_log_reg2.score(X_test, y_test)
# 0.92000000000000004
plot_decision_boundary(poly_log_reg2, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

def PolynomialLogisticRegression(degree, C):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C=C))
    ])

poly_log_reg3 = PolynomialLogisticRegression(degree=20, C=0.1)
poly_log_reg3.fit(X_train, y_train)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False))])
poly_log_reg3.score(X_train, y_train)
# 0.85333333333333339
poly_log_reg3.score(X_test, y_test)
# 0.92000000000000004
plot_decision_boundary(poly_log_reg3, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

def PolynomialLogisticRegression(degree, C, penalty='l2'):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C=C, penalty=penalty))
    ])

poly_log_reg4 = PolynomialLogisticRegression(degree=20, C=0.1, penalty='l1')
poly_log_reg4.fit(X_train, y_train)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=20, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('log_reg', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False))])
poly_log_reg4.score(X_train, y_train)
# 0.82666666666666666
poly_log_reg4.score(X_test, y_test)
# 0.90000000000000002
plot_decision_boundary(poly_log_reg4, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()



#### 9-8 OvR与OvO (19:09)
OvR 和 OvO
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:,:2]
y = iris.target
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
# LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
#           verbose=0, warm_start=False)
log_reg.score(X_test, y_test)
# 0.65789473684210531
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(log_reg, axis=[4, 8.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

log_reg2 = LogisticRegression(multi_class="multinomial", solver="newton-cg")
log_reg2.fit(X_train, y_train)
log_reg2.score(X_test, y_test)
# 0.78947368421052633
plot_decision_boundary(log_reg2, axis=[4, 8.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

# 使用所有的数据
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
# 0.94736842105263153
log_reg2 = LogisticRegression(multi_class="multinomial", solver="newton-cg")
log_reg2.fit(X_train, y_train)
log_reg2.score(X_test, y_test)
# 1.0
# OvO and OvR
from sklearn.multiclass import OneVsRestClassifier

ovr = OneVsRestClassifier(log_reg)
ovr.fit(X_train, y_train)
ovr.score(X_test, y_test)
# 0.94736842105263153
from sklearn.multiclass import OneVsOneClassifier

ovo = OneVsOneClassifier(log_reg)
ovo.fit(X_train, y_train)
ovo.score(X_test, y_test)
# 1.0


########	第10章 评价分类结果
# 极度偏斜的数据 Skewed Data， 需要引入分类准确度以外的其他的指标

10-1 准确度的陷阱和混淆矩阵 (15:06)

10-2 精准率和召回率 (11:51)

10-3 实现混淆矩阵，精准率和召回率 (15:42)
# 03-Implement-Confusion-Matrix-Precision-and-Recall.ipynb 7.0 KB
  
# 实现混淆矩阵，精准率和召回率
import numpy as np
from sklearn import datasets
digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
# 0.97555555555555551
y_log_predict = log_reg.predict(X_test)
def TN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 0))

TN(y_test, y_log_predict)
# 403
def FP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 1))

FP(y_test, y_log_predict)
# 2
def FN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 0))

FN(y_test, y_log_predict)
# 9
def TP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 1))

TP(y_test, y_log_predict)
# 36
def confusion_matrix(y_true, y_predict):
    return np.array([
        [TN(y_true, y_predict), FP(y_true, y_predict)],
        [FN(y_true, y_predict), TP(y_true, y_predict)]
    ])

confusion_matrix(y_test, y_log_predict)
# array([[403,   2],
#        [  9,  36]])
def precision_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fp = FP(y_true, y_predict)
    try:
        return tp / (tp + fp)
    except:
        return 0.0
    
precision_score(y_test, y_log_predict)
# 0.94736842105263153
def recall_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fn = FN(y_true, y_predict)
    try:
        return tp / (tp + fn)
    except:
        return 0.0
    
recall_score(y_test, y_log_predict)
# 0.80000000000000004


##	scikit-learn中的混淆矩阵，精准率和召回率
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_log_predict)
# array([[403,   2],
#        [  9,  36]])
from sklearn.metrics import precision_score

precision_score(y_test, y_log_predict)
# 0.94736842105263153
from sklearn.metrics import recall_score

recall_score(y_test, y_log_predict)
# 0.80000000000000004


####	10-4 F1 Score (17:42)
## F1 Score 是精准率和召回率的调和平均值  
# 2*recall*precision/(recall+precision)
# 1/F1Score = 1/2(1/precision + 1/recall)

# 04-F1-Score.ipynb 5.0 KB
  
# F1 Score
import numpy as np
def f1_score(precision, recall):
    try:
        return 2 * precision * recall / (precision + recall)
    except:
        return 0.0
precision = 0.5
recall = 0.5
f1_score(precision, recall)
# 0.5
precision = 0.1
recall = 0.9
f1_score(precision, recall)
# 0.18000000000000002
precision = 0.0
recall = 1.0
f1_score(precision, recall)
# 0.0
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
# 0.97555555555555551
y_predict = log_reg.predict(X_test)
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict)
# array([[403,   2],
#        [  9,  36]])
from sklearn.metrics import precision_score

precision_score(y_test, y_predict)
# 0.94736842105263153
from sklearn.metrics import recall_score

recall_score(y_test, y_predict)
# 0.80000000000000004
from sklearn.metrics import f1_score

f1_score(y_test, y_predict)
# 0.86746987951807231


####	10-5 精准率和召回率的平衡 (20:18)
## 05-Precision-Recall-Tradeoff.ipynb 7.4 KB
## 一般阈值默认为零
## 没有调整阈值的参数，可以借用log_reg.decision_function(X_test)，查看，设定阈值
	# 比如 decision_scores >= 5
  
# 精准度和召回率的平衡
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_predict = log_reg.predict(X_test)
from sklearn.metrics import f1_score

f1_score(y_test, y_predict)
# 0.86746987951807231
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict)
# array([[403,   2],
#        [  9,  36]])
from sklearn.metrics import precision_score

precision_score(y_test, y_predict)
# 0.94736842105263153
from sklearn.metrics import recall_score

recall_score(y_test, y_predict)
# 0.80000000000000004
log_reg.decision_function(X_test)[:10]
# array([-22.05700185, -33.02943631, -16.21335414, -80.37912074,
#        -48.25121102, -24.54004847, -44.39161228, -25.0429358 ,
#         -0.97827574, -19.71740779])
log_reg.predict(X_test)[:10]
# array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
decision_scores = log_reg.decision_function(X_test)
np.min(decision_scores)
# -85.686124167491727
np.max(decision_scores)
# 19.889606885682948
y_predict_2 = np.array(decision_scores >= 5, dtype='int')
confusion_matrix(y_test, y_predict_2)
# array([[404,   1],
#        [ 21,  24]])
precision_score(y_test, y_predict_2)
# 0.95999999999999996
recall_score(y_test, y_predict_2)
# 0.53333333333333333
y_predict_3 = np.array(decision_scores >= -5, dtype='int')
confusion_matrix(y_test, y_predict_3)
# array([[390,  15],
#        [  5,  40]])
precision_score(y_test, y_predict_3)
# 0.72727272727272729
recall_score(y_test, y_predict_3)
# 0.88888888888888884





####	10-6 精准率-召回率曲线 (14:21)
# 06-Precision-Recall-Curve.ipynb 47 KB
  
# 精准度-召回率曲线
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
decision_scores = log_reg.decision_function(X_test)
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

precisions = []
recalls = []
thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)
for threshold in thresholds:
    y_predict = np.array(decision_scores >= threshold, dtype='int')
    precisions.append(precision_score(y_test, y_predict))
    recalls.append(recall_score(y_test, y_predict))
plt.plot(thresholds, precisions)
plt.plot(thresholds, recalls)
plt.show()

# Precision-Recall 曲线
plt.plot(precisions, recalls)
plt.show()

scikit-learn中的Precision-Recall曲线
from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)
precisions.shape
# (145,)
recalls.shape
# (145,)
thresholds.shape
# (144,)
plt.plot(thresholds, precisions[:-1])
plt.plot(thresholds, recalls[:-1])
plt.show()

plt.plot(precisions, recalls)
plt.show()

10-7 ROC曲线 (16:52)
## Recieve Operation Characterristic Curve
## 应用于比较、选择模型（比较哪个ROC的面积大）
## 描述 TPR（Recall， TP/(TP+FN)） 和 FPR (FP/(TN+FP)) 之间的关系
## TPR和FPR相互间呈现一致的趋势

# ROC 曲线
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
decision_scores = log_reg.decision_function(X_test)
from playML.metrics import FPR, TPR

fprs = []
tprs = []
thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)
for threshold in thresholds:
    y_predict = np.array(decision_scores >= threshold, dtype='int')
    fprs.append(FPR(y_test, y_predict))
    tprs.append(TPR(y_test, y_predict))
plt.plot(fprs, tprs)
plt.show()

scikit-learn中的ROC
from sklearn.metrics import roc_curve

fprs, tprs, thresholds = roc_curve(y_test, decision_scores)
plt.plot(fprs, tprs)
plt.show()

# ROC AUC （area under curve)
from sklearn.metrics import roc_auc_score

roc_auc_score(y_test, decision_scores)
# 0.98304526748971188


####	10-8 多分类问题中的混淆矩阵 (13:30)
## 08-Confusion-Matrix-in-Multiclass-Classification.ipynb 15 KB
## 参数设定 precision_score(y_test, y_predict, average="micro")
  
# 多分类问题中的混淆矩阵
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=666)
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
# 0.93115438108484005
y_predict = log_reg.predict(X_test)
from sklearn.metrics import precision_score

precision_score(y_test, y_predict)

precision_score(y_test, y_predict, average="micro")
# 0.93115438108484005
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict)
# array([[147,   0,   1,   0,   0,   1,   0,   0,   0,   0],
#        [  0, 123,   1,   2,   0,   0,   0,   3,   4,  10],
#        [  0,   0, 134,   1,   0,   0,   0,   0,   1,   0],
#        [  0,   0,   0, 138,   0,   5,   0,   1,   5,   0],
#        [  2,   5,   0,   0, 139,   0,   0,   3,   0,   1],
#        [  1,   3,   1,   0,   0, 146,   0,   0,   1,   0],
#        [  0,   2,   0,   0,   0,   1, 131,   0,   2,   0],
#        [  0,   0,   0,   1,   0,   0,   0, 132,   1,   2],
#        [  1,   9,   2,   3,   2,   4,   0,   0, 115,   4],
#        [  0,   1,   0,   5,   0,   3,   0,   2,   2, 134]])
cfm = confusion_matrix(y_test, y_predict)
plt.matshow(cfm, cmap=plt.cm.gray)
plt.show()

row_sums = np.sum(cfm, axis=1)
err_matrix = cfm / row_sums
np.fill_diagonal(err_matrix, 0)

plt.matshow(err_matrix, cmap=plt.cm.gray)
plt.show()


########	第11章 支撑向量机 SVM
####	11-0 
	##	!! 和kNN一样需要做数据标准化（因为涉及距离计算）
	##	support vector machine
	##	SVM 尝试寻找一个最优的决策边界
	##	距离两个类别的最近的样本最远， 而最近的点被称为支撑向量，同时决策边界是由这些支撑向量所决定的
	##	SVM 任务是最大化Margin （典型的转为最优化的问题）
		# 线性可分的情况下 Hard Margin SVM
		# 实际情况下通常是线性不可分的，Soft Margin SVM，能够有一定的容错能力
	##	求解一个有条件的数学最优化问题

####	11-1 什么是SVM (13:54)

####	11-2 SVM背后的最优化问题 (19:21)

####	11-3 Soft Margin SVM (16:12)

####	11-4 scikit-learn中的SVM (21:23)
# 04-SVM-in-scikit-learn.ipynb 69 KB
  
# scikit-learn中的SVM
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()

X = iris.data
y = iris.target

X = X[y<2,:2]
y = y[y<2]
plt.scatter(X[y==0,0], X[y==0,1], color='red')
plt.scatter(X[y==1,0], X[y==1,1], color='blue')
plt.show()

from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X)
X_standard = standardScaler.transform(X)
from sklearn.svm import LinearSVC

svc = LinearSVC(C=1e9)
svc.fit(X_standard, y)
# LinearSVC(C=1000000000.0, class_weight=None, dual=True, fit_intercept=True,
#      intercept_scaling=1, loss='squared_hinge', max_iter=1000,
#      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
#      verbose=0)
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(svc, axis=[-3, 3, -3, 3])
plt.scatter(X_standard[y==0,0], X_standard[y==0,1])
plt.scatter(X_standard[y==1,0], X_standard[y==1,1])
plt.show()

svc2 = LinearSVC(C=0.01)
svc2.fit(X_standard, y)
# LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,
#      intercept_scaling=1, loss='squared_hinge', max_iter=1000,
#      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
#      verbose=0)
plot_decision_boundary(svc2, axis=[-3, 3, -3, 3])
plt.scatter(X_standard[y==0,0], X_standard[y==0,1])
plt.scatter(X_standard[y==1,0], X_standard[y==1,1])
plt.show()

svc.coef_
# array([[ 4.03243305, -2.49295041]])
svc.intercept_
# array([ 0.9536471])
def plot_svc_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
    
    w = model.coef_[0]
    b = model.intercept_[0]
    
    # w0*x0 + w1*x1 + b = 0
    # => x1 = -w0/w1 * x0 - b/w1
    plot_x = np.linspace(axis[0], axis[1], 200)
    up_y = -w[0]/w[1] * plot_x - b/w[1] + 1/w[1]
    down_y = -w[0]/w[1] * plot_x - b/w[1] - 1/w[1]
    
    up_index = (up_y >= axis[2]) & (up_y <= axis[3])
    down_index = (down_y >= axis[2]) & (down_y <= axis[3])
    plt.plot(plot_x[up_index], up_y[up_index], color='black')
    plt.plot(plot_x[down_index], down_y[down_index], color='black')
plot_svc_decision_boundary(svc, axis=[-3, 3, -3, 3])
plt.scatter(X_standard[y==0,0], X_standard[y==0,1])
plt.scatter(X_standard[y==1,0], X_standard[y==1,1])
plt.show()

plot_svc_decision_boundary(svc2, axis=[-3, 3, -3, 3])
plt.scatter(X_standard[y==0,0], X_standard[y==0,1])
plt.scatter(X_standard[y==1,0], X_standard[y==1,1])
plt.show()


####	11-5 SVM中使用多项式特征和核函数 (12:43)
## 05-Polynomial-Features-in-SVM-and-Kernel-Function.ipynb 50 KB
  
# SVM中的使用多项式特征
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons()
X.shape
# (100, 2)
y.shape
# (100,)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

X, y = datasets.make_moons(noise=0.15, random_state=666)

plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# 使用多项式特征的SVM
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

def PolynomialSVC(degree, C=1.0):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("linearSVC", LinearSVC(C=C))
    ])
poly_svc = PolynomialSVC(degree=3)
poly_svc.fit(X, y)
# Pipeline(steps=[('poly', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearSVC', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
#      intercept_scaling=1, loss='squared_hinge', max_iter=1000,
#      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
#      verbose=0))])
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(poly_svc, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

##! 使用多项式核函数的SVM
from sklearn.svm import SVC

def PolynomialKernelSVC(degree, C=1.0):
    return Pipeline([
        ("std_scaler", StandardScaler()),
        ("kernelSVC", SVC(kernel="poly", degree=degree, C=C))
    ])
poly_kernel_svc = PolynomialKernelSVC(degree=3)
poly_kernel_svc.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('kernelSVC', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma='auto', kernel='poly',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(poly_kernel_svc, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

####	11-6 到底什么是核函数 (15:31)

####	11-7 RBF核函数 (20:46)
## RBF函数 = 高斯函数 = 径向基函数
## 高斯核函数的本质：将每一个样本点映射到一个无穷维的特征空间
	## （前写章的）多项式特征：依靠升维使原本线性不可分的数据线性可分，高斯核函数本质上也是如此
## 对于每个点都是Landmark m*n的数据映射成了m*m的数据
## 应用：样本远远小于变量（m<n）-> 自安语言处理
## 07-What-is-RBF-Kernel.ipynb 15 KB
  
# 直观理解高斯核函数
import numpy as np
import matplotlib.pyplot as plt
x = np.arange(-4, 5, 1)
x
# array([-4, -3, -2, -1,  0,  1,  2,  3,  4])
y = np.array((x >= -2) & (x <= 2), dtype='int')
y
# array([0, 0, 1, 1, 1, 1, 1, 0, 0])
plt.scatter(x[y==0], [0]*len(x[y==0]))
plt.scatter(x[y==1], [0]*len(x[y==1]))
plt.show()

def gaussian(x, l):
    gamma = 1.0
    return np.exp(-gamma * (x-l)**2)
l1, l2 = -1, 1

X_new = np.empty((len(x), 2))
for i, data in enumerate(x):
    X_new[i, 0] = gaussian(data, l1)
    X_new[i, 1] = gaussian(data, l2)
plt.scatter(X_new[y==0,0], X_new[y==0,1])
plt.scatter(X_new[y==1,0], X_new[y==1,1])
plt.show()

####	11-8 RBF核函数中的gamma (13:39)
## 08-RBF-Kernel-in-scikit-learn.ipynb 76 KB
  
scikit-learn 中的 RBF 核
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(noise=0.15, random_state=666)

plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC

def RBFKernelSVC(gamma):
    return Pipeline([
        ("std_scaler", StandardScaler()),
        ("svc", SVC(kernel="rbf", gamma=gamma))
    ])
svc = RBFKernelSVC(gamma=1)
svc.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=1, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(svc, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

svc_gamma100 = RBFKernelSVC(gamma=100)
svc_gamma100.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=100, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(svc_gamma100, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

svc_gamma10 = RBFKernelSVC(gamma=10)
svc_gamma10.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=10, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(svc_gamma10, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

svc_gamma05 = RBFKernelSVC(gamma=0.5)
svc_gamma05.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(svc_gamma05, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

svc_gamma01 = RBFKernelSVC(gamma=0.1)
svc_gamma01.fit(X, y)
# Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#   decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',
#   max_iter=-1, probability=False, random_state=None, shrinking=True,
#   tol=0.001, verbose=False))])
plot_decision_boundary(svc_gamma01, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()


####	11-9 SVM思想解决回归问题 (10:46)
## 09-SVM-Regressor.ipynb 2.8 KB
  
# SVM 思想解决回归问题
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
from sklearn.svm import LinearSVR
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def StandardLinearSVR(epsilon=0.1):
    return Pipeline([
        ('std_scaler', StandardScaler()),
        ('linearSVR', LinearSVR(epsilon=epsilon))
    ])
svr = StandardLinearSVR()
svr.fit(X_train, y_train)
Pipeline(steps=[('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearSVR', LinearSVR(C=1.0, dual=True, epsilon=0.1, fit_intercept=True,
     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,
     random_state=None, tol=0.0001, verbose=0))])
svr.score(X_test, y_test)
# 0.63618523213237332


########	第12章 决策树
#### 12-0
	## 非参数学习算法
	## 天然地可以解决 分类、回归问题

#### 12-1 什么是决策树 (12:01)
## 01-What-is-Decision-Tree.ipynb 22 KB
  
# 什么是决策树
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:,2:]
y = iris.target
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy", random_state=42)
dt_clf.fit(X, y)
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=42,
            splitter='best')
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, cmap=custom_cmap)
plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

####	12-2 信息熵 (16:30)
## 熵越大，数据的不确定性更高
## 熵越小，数据的不确定性更低
## 02-Entropy.ipynb 16 KB
  
# 信息熵
# 二分类问题的函数
import numpy as np
import matplotlib.pyplot as plt
def entropy(p):
    return -p * np.log(p) - (1-p) * np.log(1-p)
x = np.linspace(0.01, 0.99, 200)
plt.plot(x, entropy(x))
plt.show()

####	12-3 使用信息熵寻找最优划分 (20:20)

####	12-4 基尼系数 (17:41)
# 熵信息计算比基尼系数稍慢
# sklearn默认为基尼系数
# 两者大多数时候没有太大的区别


####	12-5 CART与决策树中的超参数 (15:04)
## 05-CART-and-Decision-Tree-Hyperparameters.ipynb 68 KB
  
# CART 和 决策树的超参数
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(noise=0.25, random_state=666)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier()
dt_clf.fit(X, y)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
plot_decision_boundary(dt_clf, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

dt_clf2 = DecisionTreeClassifier(max_depth=2)
dt_clf2.fit(X, y)

plot_decision_boundary(dt_clf2, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

dt_clf3 = DecisionTreeClassifier(min_samples_split=10)
dt_clf3.fit(X, y)

plot_decision_boundary(dt_clf3, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

dt_clf4 = DecisionTreeClassifier(min_samples_leaf=6)
dt_clf4.fit(X, y)

plot_decision_boundary(dt_clf4, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

dt_clf5 = DecisionTreeClassifier(max_leaf_nodes=4)
dt_clf5.fit(X, y)

plot_decision_boundary(dt_clf5, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# SkLearn -> Cart -> Classification And Regression Tree
# 其他 ID3, C4.5, C5.0
# 复杂度：O(logm0)、训练：O(n*m*logm) 
# 容易过拟合（所有非参数学习的特点）-> 需要剪枝、解决过拟合


####	12-6 决策树解决回归问题 (08:15)
#	06-Decision-Tree-Regressor.ipynb 2.8 KB
  
# 决策树解决回归问题
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor

dt_reg = DecisionTreeRegressor()
dt_reg.fit(X_train, y_train)
DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,
           max_leaf_nodes=None, min_impurity_decrease=0.0,
           min_impurity_split=None, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           presort=False, random_state=None, splitter='best')
dt_reg.score(X_test, y_test)
# 0.58605479243964098
dt_reg.score(X_train, y_train)
# 1.0   -> 显示了非常容易过拟合

####	12-7 决策树的局限性 (08:16)
# 横平竖直
# 对个别特殊数据敏感

########	第13章 集成学习和随机森林
####	13-1 什么是集成学习 (16:35)
# kNN
# 逻辑回归
# 决策树
# 神经网络
# 贝叶斯
# -> 投票决策 voting classifer

#	01-What-is-Ensemble-Learning.ipynb 28 KB
  
# 什么是集成学习
# hard voting
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression()
log_clf.fit(X_train, y_train)
log_clf.score(X_test, y_test)
0.86399999999999999
from sklearn.svm import SVC

svm_clf = SVC()
svm_clf.fit(X_train, y_train)
svm_clf.score(X_test, y_test)
0.88800000000000001
from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(random_state=666)
dt_clf.fit(X_train, y_train)
dt_clf.score(X_test, y_test)
0.86399999999999999
y_predict1 = log_clf.predict(X_test)
y_predict2 = svm_clf.predict(X_test)
y_predict3 = dt_clf.predict(X_test)
y_predict = np.array((y_predict1 + y_predict2 + y_predict3) >= 2, dtype='int')
y_predict[:10]
array([1, 0, 0, 1, 1, 1, 0, 0, 0, 0])
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_predict)
0.89600000000000002
使用Voting Classifier
from sklearn.ensemble import VotingClassifier

voting_clf = VotingClassifier(estimators=[
    ('log_clf', LogisticRegression()), 
    ('svm_clf', SVC()),
    ('dt_clf', DecisionTreeClassifier(random_state=666))],
                             voting='hard')
voting_clf.fit(X_train, y_train)
voting_clf.score(X_test, y_test)
0.89600000000000002

####	13-2 Soft Voting Classifier (14:30)
# SVC -> 设置 probability=True
# 算法再多，种类也是有限的（投票者还是太少） -> 随机森林
# 使用 Soft Voting Classifier

voting_clf2 = VotingClassifier(estimators=[
    ('log_clf', LogisticRegression()), 
    ('svm_clf', SVC(probability=True)),
    ('dt_clf', DecisionTreeClassifier(random_state=666))],
                             voting='soft')
voting_clf2.fit(X_train, y_train)
voting_clf2.score(X_test, y_test)
0.91200000000000003

####	13-3 Bagging 和 Pasting (16:52)

#	03-Bagging-and-Pasting.ipynb 26 KB
# 创建更多的子模型！ 集成更多的意见、投票者
# 子模型之间不能一致！ 子模型间要有差异性
	# 每个模型只看样本的一部分（如：500个只看100个）
	# 每个子模型并需要太高的准确率

# Bagging 和 Pasting
# 放回取样（bagging）-> 更常用 统计学中：bootstrap， bootstrap=True：放回取样
# 不放回曲阳（pasting）
# 使用决策树型模型，这种非参数学习的方式，更能产生差异比较大的子模型

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) 
使用 Bagging
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                           n_estimators=500, max_samples=100,
                           bootstrap=True)
bagging_clf.fit(X_train, y_train)
bagging_clf.score(X_test, y_test)
0.91200000000000003
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                           n_estimators=5000, max_samples=100,
                           bootstrap=True)
bagging_clf.fit(X_train, y_train)
bagging_clf.score(X_test, y_test)
0.92000000000000004


###	13-4 oob (Out-of-Bag) 和关于Bagging的更多讨论 (14:40)
#	放回取样导致一部分样本很多可能没有取到
#	平均大约有37%的样本没有取到 out-of-bag
#   不需要测试数据集
#	oob_score_

#	更多探讨
	# 针对特征进行随机采样（Random Subspaces）
	# 即针对样本，有针对性特征（Random Patches）

##	04-OOB-and-More-about-Bagging-Classifier.ipynb 29 KB
  
# oob和更多Bagging相关
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42) 
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# oob
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=100,
                               bootstrap=True, oob_score=True)
bagging_clf.fit(X, y)
BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best'),
         bootstrap=True, bootstrap_features=False, max_features=1.0,
         max_samples=100, n_estimators=500, n_jobs=1, oob_score=True,
         random_state=None, verbose=0, warm_start=False)
bagging_clf.oob_score_
0.91800000000000004
# n_jobs
%%time
bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=100,
                               bootstrap=True, oob_score=True)
bagging_clf.fit(X, y)
CPU times: user 1.81 s, sys: 27.2 ms, total: 1.84 s
Wall time: 2.95 s
%%time
bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=100,
                               bootstrap=True, oob_score=True,
                               n_jobs=-1)
bagging_clf.fit(X, y)
CPU times: user 385 ms, sys: 56.1 ms, total: 441 ms
Wall time: 1.83 s

##	bootstrap_features （特征随机取样）
random_subspaces_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=500,
                               bootstrap=True, oob_score=True,
                               max_features=1, bootstrap_features=True)
random_subspaces_clf.fit(X, y)
random_subspaces_clf.oob_score_
0.83399999999999996
random_patches_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=100,
                               bootstrap=True, oob_score=True,
                               max_features=1, bootstrap_features=True)
random_patches_clf.fit(X, y)
random_patches_clf.oob_score_
0.85799999999999998


####	13-5 随机森林和 Extra-Trees (13:14)
# Bagging 、BaseEsimator（Decision Tree） -> 随机森林

#	05-Random-Forest-and-Extra-Trees.ipynb 29 KB
  
# 随机森林
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=666)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

# 随机森林
from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(n_estimators=500, oob_score=True, random_state=666, n_jobs=-1)
rf_clf.fit(X, y)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,
            oob_score=True, random_state=666, verbose=0, warm_start=False)
rf_clf.oob_score_
0.89200000000000002
rf_clf2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, oob_score=True, random_state=666, n_jobs=-1)
rf_clf2.fit(X, y)
rf_clf2.oob_score_
0.90600000000000003
# 随机森林拥有决策树和BaggingClassifier的所有参数：）

# Extra-Trees
from sklearn.ensemble import ExtraTreesClassifier

et_clf = ExtraTreesClassifier(n_estimators=500, bootstrap=True, oob_score=True, random_state=666, n_jobs=-1)
et_clf.fit(X, y)
ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',
           max_depth=None, max_features='auto', max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,
           oob_score=True, random_state=666, verbose=0, warm_start=False)
et_clf.oob_score_
0.89200000000000002

# 集成学习解决回归问题
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor

####	13-6 Ada Boosting 和 Gradient Boosting (15:30)
#	集成多个模型
#	每个模型都在尝试增强（Boosting）整体的效果
#	数据 -> 模型1 -> 结果 -> 模型2 -> 结果 -> 模型 ... -> 结果



##	06-AdaBoost-and-Gradient-Boosting.ipynb 29 KB
# Boosting
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=666) 
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
AdaBoosting
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=2), n_estimators=500)
ada_clf.fit(X_train, y_train)
AdaBoostClassifier(algorithm='SAMME.R',
          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best'),
          learning_rate=1.0, n_estimators=500, random_state=None)
ada_clf.score(X_test, y_test)
0.85599999999999998

##	Gradient Boosting -> 不断修正error值
# Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier

gb_clf = GradientBoostingClassifier(max_depth=2, n_estimators=30)
gb_clf.fit(X_train, y_train)
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=2,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=30,
              presort='auto', random_state=None, subsample=1.0, verbose=0,
              warm_start=False)
gb_clf.score(X_test, y_test)
0.90400000000000003



# Boosting 解决回归问题
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor

####	13-7 Stacking (08:46)

########	第14章 更多机器学习算法
####	14-1 学习scikit-learn文档, 大家加油！ (08:32)
# UserGuide：ttps://scikit-learn.org/stable/user_guide.html
# API接口：https://scikit-learn.org/stable/modules/classes.html

####	14-2 学习完这个课程以后怎样继续深入机器学习的学习？
